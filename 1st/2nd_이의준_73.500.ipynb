{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydfpk1tQRtPL"
      },
      "source": [
        "## **Install Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xhKi0dBPraj",
        "outputId": "88f596fb-9685-483a-8066-0df5f4eee2a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20 kB 40.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 61 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 71 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 81 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 92 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 102 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 112 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 122 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 133 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 143 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 153 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 163 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 174 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 184 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 194 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 204 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 215 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 225 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 235 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 245 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 256 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 266 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 276 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 286 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 296 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 307 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 317 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 327 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 337 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 348 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 358 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 368 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 378 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 389 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 399 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 409 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 419 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 430 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 440 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 450 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 460 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 471 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 481 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 491 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 501 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 509 kB 24.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install timm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pBgBRrlSZSR"
      },
      "source": [
        "## **Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCASWFlDQVVM",
        "outputId": "54a3f16b-b0bf-409e-a935-3fc766e157d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ7O-OaQSOlZ"
      },
      "source": [
        "## **Import Library & Set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f-YpDDTiRPN8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!cp /content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/data/data.zip .\n",
        "!cp /content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/sample_submission.csv .\n",
        "!unzip -qq data.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seed(random_state=0):\n",
        "  torch.manual_seed(random_state)\n",
        "  torch.cuda.manual_seed(random_state)\n",
        "  np.random.seed(random_state)\n",
        "  random.seed(random_state)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "QJfBM-wNW3YM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i8EV7UrZHHI"
      },
      "source": [
        "## **Data Preprocessing**\n",
        "1. Define Dataset Class\n",
        "2. Add Data Augmentation\n",
        "3. Make Train/Valid Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfP1ywU7VrV0"
      },
      "source": [
        "### Define Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "McchZTBWeAJd"
      },
      "outputs": [],
      "source": [
        "class CustomSubset(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.indices = self.subset.indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.subset[idx]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root='./test', transform=None):\n",
        "        self.root = root\n",
        "        self.data = os.listdir(self.root)\n",
        "        self.data = sorted(self.data)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(os.path.join(self.root, self.data[idx]))\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxc9Zl_wXDHO"
      },
      "source": [
        "### Add Data Augmentation : Cutout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "4XzlCIC4ctUj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXU45uoTZrMP"
      },
      "source": [
        "### Make Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.utils.data import Subset # create two datasets from all_data\n",
        "\n",
        "def get_loaders(path='./train', batch_size=32, n_splits=1, test_size=0.1, random_state=0, transform=None):\n",
        "    if transform is None:\n",
        "        transform = {'train' : None, 'valid' : None}\n",
        "        \n",
        "    all_data = ImageFolder(root='./train')\n",
        "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    indices = list(range(len(all_data)))\n",
        "    y_all_data = [y for _, y in all_data]\n",
        "\n",
        "    for train_index, val_index in sss.split(indices, y_all_data):\n",
        "        pass\n",
        "\n",
        "    train_subset = Subset(all_data, train_index)\n",
        "    test_subset = Subset(all_data, val_index)\n",
        "\n",
        "    train_data = CustomSubset(train_subset, transform['train'])\n",
        "    test_data = CustomSubset(test_subset, transform['valid'])\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    return all_data, train_data, train_loader, test_loader"
      ],
      "metadata": {
        "id": "We8T7nz3XU5B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTd9zUPGa8JX"
      },
      "source": [
        "## **Model Parameter Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D5YCfy1Fxnd1"
      },
      "outputs": [],
      "source": [
        "# for checking the number of parameters\n",
        "# do not modify below\n",
        "def param_check(model):\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        total += torch.numel(param)\n",
        "\n",
        "    print(f'# of parameters : {total}\\n')\n",
        "    return total < 15000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIrokqaUbttv"
      },
      "source": [
        "## **Define Main Function**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader, valid_loader, num_epochs, device, print_every=10, scheduler=None, path2weight=None):\n",
        "    if path2weight == None:\n",
        "      raise Exception(\"model의 가중치 파일을 저장할 경로를 지정해주세요.\")\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    best_acc = -np.inf\n",
        "    start_time = datetime.now()\n",
        "    try:\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = 0\n",
        "            train_acc = 0\n",
        "            processed = 0\n",
        "            model.train()\n",
        "            # training step\n",
        "            print(f'Epoch [{epoch}] Learning Rate : {get_lr(optimizer)}')\n",
        "            for i, (data, label) in enumerate(train_loader):\n",
        "                data, label = data.to(device), label.to(device)\n",
        "                pred = model(data)\n",
        "                loss = criterion(pred, label)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * data.size(0)\n",
        "                train_acc += torch.sum(pred.argmax(1) == label).item()\n",
        "                processed += data.size(0)\n",
        "                if (i % print_every == 0) or (i == len(train_loader) - 1):\n",
        "                    print(\n",
        "                        f'Epoch [{epoch}/{num_epochs - 1}]({(i + 1) / len(train_loader) * 100:.2f}%)\\t\\\n",
        "                        train loss: {train_loss / processed:.4f}\\t\\\n",
        "                        train acc: {train_acc / processed * 100:.4f}%\\t\\\n",
        "                        time elapsed: {datetime.now() - start_time}'\n",
        "                        )\n",
        "            train_losses.append(train_loss / processed)\n",
        "            train_accs.append(train_acc / processed)\n",
        "            print(\"=\"*100)\n",
        "\n",
        "            # validation step\n",
        "            valid_loss = 0\n",
        "            valid_acc = 0\n",
        "            processed = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for i, (data, label) in enumerate(valid_loader):\n",
        "                    data, label = data.to(device), label.to(device)\n",
        "                    pred = model(data)\n",
        "                    loss = criterion(pred, label)\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "                    valid_acc += torch.sum(pred.argmax(1) == label).item()\n",
        "                    processed += data.size(0)\n",
        "                    if (i % print_every == 0) or (i == len(valid_loader) - 1):\n",
        "                        print(\n",
        "                            f'Epoch [{epoch}/{num_epochs - 1}]({(i + 1) / len(valid_loader) * 100:.2f}%)\\t\\\n",
        "                            valid loss: {valid_loss / processed:.4f}\\t\\\n",
        "                            valid acc: {valid_acc / processed * 100:.4f}%\\t\\\n",
        "                            time elapsed: {datetime.now() - start_time}'\n",
        "                            )\n",
        "                if valid_acc / processed > best_acc:\n",
        "                    best_acc = valid_acc / processed\n",
        "                    torch.save(model.state_dict(), f'{path2weight}_best_gc01.pth')\n",
        "                    print(\"Best weight saved!!\")\n",
        "                \n",
        "                torch.save(model.state_dict(), f'{path2weight}_last_gc01.pth')\n",
        "                valid_losses.append(valid_loss / processed)\n",
        "                valid_accs.append(valid_acc / processed)\n",
        "            print(\"=\"*100)\n",
        "            if scheduler is not None:\n",
        "                scheduler.step(best_acc)\n",
        "        return train_losses, train_accs, valid_losses, valid_accs, best_acc\n",
        "    except KeyboardInterrupt:\n",
        "        return train_losses, train_accs, valid_losses, valid_accs, best_acc"
      ],
      "metadata": {
        "id": "IRClCsc5dnHV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(cfg=None, trial=None):\n",
        "  if cfg is None:\n",
        "    cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            'batch_size' : 128,\n",
        "            'n_epochs' : 85,\n",
        "            'seed' : 121,\n",
        "            'n_splits' : 1,\n",
        "            'test_size' : 0.05,\n",
        "            'cutout' : True,\n",
        "            'model_name' : 'efficientnet_lite0',\n",
        "            'num_classes' : 120,\n",
        "            'log_interval' : 50,\n",
        "            #'save_model' : False,\n",
        "            'lr' : 0.0033,\n",
        "            'weight_decay' :  5.3e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer' : optim.Adam,\n",
        "            'scheduler' : optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            'path2weight' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/weights/',\n",
        "            'path2submit' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/',\n",
        "          }\n",
        "\n",
        "  ######################################## train ########################################\n",
        "  transform = {\n",
        "    'train': transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                 #transforms.CenterCrop((256)),\n",
        "                                 transforms.RandomHorizontalFlip(),\n",
        "                                 transforms.RandomRotation(10),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "             \n",
        "    'valid': transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                # transforms.CenterCrop((112, 112)),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "  }\n",
        "  if cfg['cutout']:\n",
        "    transform['train'].transforms.append(Cutout(n_holes=2, length=48))\n",
        "\n",
        "  try:\n",
        "    set_random_seed(cfg['seed'])\n",
        "    _, _, train_loader, valid_loader = get_loaders(batch_size=cfg['batch_size'], n_splits=cfg['n_splits'], test_size=cfg['test_size'], transform = transform)\n",
        "    model = timm.create_model(cfg['model_name'], pretrained=False, num_classes=cfg['num_classes']).to(cfg['device'])\n",
        "\n",
        "    param_check(model)\n",
        "\n",
        "    optimizer = cfg['optimizer'](model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
        "    criterion = nn.CrossEntropyLoss().to(cfg['device'])\n",
        "    scheduler = cfg['scheduler'](optimizer, mode='max', factor=0.5, patience=6, verbose=True)   # mode='max'로 해야하지않나?, metric: accr대신 loss로 하자.\n",
        "\n",
        "    # PATH Setting\n",
        "    path2weight = cfg['path2weight']+cfg['model_name']\n",
        "    path2submit = cfg['path2submit']+cfg['model_name']+'_gc01.csv'\n",
        "\n",
        "    train_losses, train_accs, valid_losses, valid_accs, best_acc \\\n",
        "    = train(model, criterion, optimizer, train_loader, valid_loader,num_epochs=cfg['n_epochs'],\\\n",
        "            device=cfg['device'], print_every=cfg['log_interval'], scheduler=scheduler, path2weight=path2weight)\n",
        "        \n",
        "    ####################################### test ########################################\n",
        "    test_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                         #transforms.CenterCrop((112, 112)),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "    test_data = TestDataset(root='./test', transform=test_transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=cfg['batch_size'], shuffle=False)\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{path2weight}_best_gc01.pth'))\n",
        "    model.to(cfg['device'])\n",
        "    preds = test(model, test_loader, cfg['device'])\n",
        "    submission = pd.read_csv('sample_submission.csv')\n",
        "    submission['Category'] = preds.cpu().numpy()\n",
        "    submission.to_csv(path2submit, index=False)\n",
        "\n",
        "    return train_losses, train_accs, valid_losses, valid_accs\n",
        "  except KeyboardInterrupt:\n",
        "    return train_losses, train_accs, valid_losses, valid_accs"
      ],
      "metadata": {
        "id": "E3ugwRRvZdHv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model1: test_acc 65.29%"
      ],
      "metadata": {
        "id": "dXCKWKylAIt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            'batch_size' : 128,\n",
        "            'n_epochs' : 110,\n",
        "            'seed' : 666,\n",
        "            'n_splits' : 1,\n",
        "            'test_size' : 0.1,\n",
        "            'cutout' : True,\n",
        "            'model_name' : 'efficientnet_lite0',\n",
        "            'num_classes' : 120,\n",
        "            'log_interval' : 50,\n",
        "            #'save_model' : False,\n",
        "            'lr' : 0.005,\n",
        "            'weight_decay' :  5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer' : optim.Adam,\n",
        "            'scheduler' : optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            'path2weight' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/weights/',\n",
        "            'path2submit' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/',\n",
        "          }\n",
        "\n",
        "# 변경 1\n",
        "# main() 함수의 scheduler parameter가 아래와 같음\n",
        "# scheduler = cfg['scheduler'](optimizer, mode='min', factor=0.55, patience=7, verbose=True)\n",
        "\n",
        "# 변경 2\n",
        "# train() 함수의 74줄이 아래와 같이 변경됨\n",
        "# scheduler.step(valid_loss / processed)\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = main(cfg=cfg)"
      ],
      "metadata": {
        "id": "0Y5se_41AW1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model2: valid_acc 67.3817%"
      ],
      "metadata": {
        "id": "eHhc0EwkAIjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            'batch_size' : 128,\n",
        "            'n_epochs' : 100,\n",
        "            'seed' : 0,\n",
        "            'n_splits' : 1,\n",
        "            'test_size' : 0.1,\n",
        "            'cutout' : True,\n",
        "            'model_name' : 'efficientnet_lite0',\n",
        "            'num_classes' : 120,\n",
        "            'log_interval' : 50,\n",
        "            #'save_model' : False,\n",
        "            'lr' : 0.003,\n",
        "            'weight_decay' :  5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer' : optim.Adam,\n",
        "            'scheduler' : optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            'path2weight' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/weights/',\n",
        "            'path2submit' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/',\n",
        "          }\n",
        "\n",
        "# 변경 1\n",
        "# main() 함수의 transform에서 cutout의 parameter가 아래와 같음\n",
        "# transform['train'].transforms.append(Cutout(n_holes=2, length=48))\n",
        "\n",
        "# 변경 2\n",
        "# main() 함수의 scheduler parameter가 아래와 같음\n",
        "# scheduler = cfg['scheduler'](optimizer, mode='min', factor=0.5, patience=6, verbose=True)\n",
        "\n",
        "# 변경 3\n",
        "# train() 함수의 74줄이 아래와 같이 변경됨\n",
        "# scheduler.step(valid_loss / processed)\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = main(cfg=cfg)"
      ],
      "metadata": {
        "id": "uQUcWSG_AooF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model3: valid_acc 66.7767%"
      ],
      "metadata": {
        "id": "yjM7DMA2AIVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            'batch_size' : 128,\n",
        "            'n_epochs' : 100,\n",
        "            'seed' : 0,\n",
        "            'n_splits' : 1,\n",
        "            'test_size' : 0.05,\n",
        "            'cutout' : True,\n",
        "            'model_name' : 'efficientnet_lite0',\n",
        "            'num_classes' : 120,\n",
        "            'log_interval' : 50,\n",
        "            #'save_model' : False,\n",
        "            'lr' : 0.0033,\n",
        "            'weight_decay' :  5.3e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer' : optim.Adam,\n",
        "            'scheduler' : optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            'path2weight' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/weights/',\n",
        "            'path2submit' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/',\n",
        "          }\n",
        "\n",
        "# 변경 1\n",
        "# main() 함수의 transform에서 cutout의 parameter가 아래와 같음\n",
        "# transform['train'].transforms.append(Cutout(n_holes=3, length=36))\n",
        "\n",
        "# 변경 2\n",
        "# main() 함수의 scheduler parameter가 아래와 같음\n",
        "# scheduler = cfg['scheduler'](optimizer, mode='min', factor=0.5, patience=6, verbose=True)\n",
        "\n",
        "# 변경 3\n",
        "# train() 함수의 74줄이 아래와 같이 변경됨\n",
        "# scheduler.step(valid_loss / processed)\n",
        "\n",
        "train_losses, train_accs, valid_losses, valid_accs = main(cfg=cfg)"
      ],
      "metadata": {
        "id": "CVcYL2y8A7kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model4: valid_acc 73.5974%"
      ],
      "metadata": {
        "id": "0EzhMKsLAAlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_accs, valid_losses, valid_accs = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6g4fk6OTYIk",
        "outputId": "658a751b-065d-4b6a-9359-1bf3bdc75b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of parameters : 3524728\n",
            "\n",
            "Epoch [0] Learning Rate : 0.0033\n",
            "Epoch [0/84](0.74%)\t                        train loss: 4.9289\t                        train acc: 0.0000%\t                        time elapsed: 0:00:08.571013\n",
            "Epoch [0/84](37.78%)\t                        train loss: 5.0250\t                        train acc: 2.2978%\t                        time elapsed: 0:01:10.779132\n",
            "Epoch [0/84](74.81%)\t                        train loss: 4.6920\t                        train acc: 3.3957%\t                        time elapsed: 0:02:09.556019\n",
            "Epoch [0/84](100.00%)\t                        train loss: 4.5717\t                        train acc: 3.9199%\t                        time elapsed: 0:02:50.723688\n",
            "====================================================================================================\n",
            "Epoch [0/84](12.50%)\t                            valid loss: 4.6259\t                            valid acc: 5.4688%\t                            time elapsed: 0:02:51.433722\n",
            "Epoch [0/84](100.00%)\t                            valid loss: 4.5956\t                            valid acc: 2.2002%\t                            time elapsed: 0:02:55.790536\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [1] Learning Rate : 0.0033\n",
            "Epoch [1/84](0.74%)\t                        train loss: 4.0391\t                        train acc: 7.8125%\t                        time elapsed: 0:02:58.843334\n",
            "Epoch [1/84](37.78%)\t                        train loss: 4.0605\t                        train acc: 6.8168%\t                        time elapsed: 0:03:59.316648\n",
            "Epoch [1/84](74.81%)\t                        train loss: 4.0127\t                        train acc: 7.2324%\t                        time elapsed: 0:04:58.034339\n",
            "Epoch [1/84](100.00%)\t                        train loss: 3.9795\t                        train acc: 7.4692%\t                        time elapsed: 0:05:39.045323\n",
            "====================================================================================================\n",
            "Epoch [1/84](12.50%)\t                            valid loss: 4.4522\t                            valid acc: 6.2500%\t                            time elapsed: 0:05:39.747859\n",
            "Epoch [1/84](100.00%)\t                            valid loss: 4.3921\t                            valid acc: 4.7305%\t                            time elapsed: 0:05:44.016224\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [2] Learning Rate : 0.0033\n",
            "Epoch [2/84](0.74%)\t                        train loss: 3.8948\t                        train acc: 11.7188%\t                        time elapsed: 0:05:45.560905\n",
            "Epoch [2/84](37.78%)\t                        train loss: 3.8359\t                        train acc: 9.3750%\t                        time elapsed: 0:06:44.157452\n",
            "Epoch [2/84](74.81%)\t                        train loss: 3.7889\t                        train acc: 9.5684%\t                        time elapsed: 0:07:43.416761\n",
            "Epoch [2/84](100.00%)\t                        train loss: 3.7626\t                        train acc: 10.1847%\t                        time elapsed: 0:08:23.323473\n",
            "====================================================================================================\n",
            "Epoch [2/84](12.50%)\t                            valid loss: 4.1314\t                            valid acc: 8.5938%\t                            time elapsed: 0:08:24.047472\n",
            "Epoch [2/84](100.00%)\t                            valid loss: 4.1192\t                            valid acc: 7.0407%\t                            time elapsed: 0:08:28.360268\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [3] Learning Rate : 0.0033\n",
            "Epoch [3/84](0.74%)\t                        train loss: 3.5695\t                        train acc: 12.5000%\t                        time elapsed: 0:08:29.927546\n",
            "Epoch [3/84](37.78%)\t                        train loss: 3.5966\t                        train acc: 12.3162%\t                        time elapsed: 0:09:29.992417\n",
            "Epoch [3/84](74.81%)\t                        train loss: 3.5649\t                        train acc: 12.6083%\t                        time elapsed: 0:10:29.305227\n",
            "Epoch [3/84](100.00%)\t                        train loss: 3.5421\t                        train acc: 12.8829%\t                        time elapsed: 0:11:08.775860\n",
            "====================================================================================================\n",
            "Epoch [3/84](12.50%)\t                            valid loss: 3.6933\t                            valid acc: 11.7188%\t                            time elapsed: 0:11:09.478836\n",
            "Epoch [3/84](100.00%)\t                            valid loss: 3.6732\t                            valid acc: 11.0011%\t                            time elapsed: 0:11:13.741894\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [4] Learning Rate : 0.0033\n",
            "Epoch [4/84](0.74%)\t                        train loss: 3.4196\t                        train acc: 14.8438%\t                        time elapsed: 0:11:15.357785\n",
            "Epoch [4/84](37.78%)\t                        train loss: 3.3680\t                        train acc: 15.9314%\t                        time elapsed: 0:12:14.787504\n",
            "Epoch [4/84](74.81%)\t                        train loss: 3.3799\t                        train acc: 15.8184%\t                        time elapsed: 0:13:13.096508\n",
            "Epoch [4/84](100.00%)\t                        train loss: 3.3622\t                        train acc: 15.8995%\t                        time elapsed: 0:13:53.815315\n",
            "====================================================================================================\n",
            "Epoch [4/84](12.50%)\t                            valid loss: 3.5197\t                            valid acc: 10.1562%\t                            time elapsed: 0:13:54.522363\n",
            "Epoch [4/84](100.00%)\t                            valid loss: 3.4951\t                            valid acc: 13.8614%\t                            time elapsed: 0:13:58.802656\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [5] Learning Rate : 0.0033\n",
            "Epoch [5/84](0.74%)\t                        train loss: 3.3483\t                        train acc: 17.1875%\t                        time elapsed: 0:14:00.335329\n",
            "Epoch [5/84](37.78%)\t                        train loss: 3.1508\t                        train acc: 19.7610%\t                        time elapsed: 0:14:59.014611\n",
            "Epoch [5/84](74.81%)\t                        train loss: 3.1200\t                        train acc: 20.1810%\t                        time elapsed: 0:15:58.330218\n",
            "Epoch [5/84](100.00%)\t                        train loss: 3.0995\t                        train acc: 20.3231%\t                        time elapsed: 0:16:38.711758\n",
            "====================================================================================================\n",
            "Epoch [5/84](12.50%)\t                            valid loss: 3.8726\t                            valid acc: 11.7188%\t                            time elapsed: 0:16:39.418574\n",
            "Epoch [5/84](100.00%)\t                            valid loss: 3.6805\t                            valid acc: 11.9912%\t                            time elapsed: 0:16:43.683973\n",
            "====================================================================================================\n",
            "Epoch [6] Learning Rate : 0.0033\n",
            "Epoch [6/84](0.74%)\t                        train loss: 3.2266\t                        train acc: 19.5312%\t                        time elapsed: 0:16:44.980718\n",
            "Epoch [6/84](37.78%)\t                        train loss: 2.9661\t                        train acc: 22.8554%\t                        time elapsed: 0:17:43.756490\n",
            "Epoch [6/84](74.81%)\t                        train loss: 2.9303\t                        train acc: 23.6541%\t                        time elapsed: 0:18:42.951490\n",
            "Epoch [6/84](100.00%)\t                        train loss: 2.9198\t                        train acc: 23.8261%\t                        time elapsed: 0:19:22.574107\n",
            "====================================================================================================\n",
            "Epoch [6/84](12.50%)\t                            valid loss: 3.8705\t                            valid acc: 14.8438%\t                            time elapsed: 0:19:23.275983\n",
            "Epoch [6/84](100.00%)\t                            valid loss: 3.8694\t                            valid acc: 14.0814%\t                            time elapsed: 0:19:27.609808\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [7] Learning Rate : 0.0033\n",
            "Epoch [7/84](0.74%)\t                        train loss: 2.7687\t                        train acc: 30.4688%\t                        time elapsed: 0:19:29.168135\n",
            "Epoch [7/84](37.78%)\t                        train loss: 2.7210\t                        train acc: 27.6195%\t                        time elapsed: 0:20:28.770828\n",
            "Epoch [7/84](74.81%)\t                        train loss: 2.7324\t                        train acc: 27.3360%\t                        time elapsed: 0:21:28.241838\n",
            "Epoch [7/84](100.00%)\t                        train loss: 2.7249\t                        train acc: 27.3117%\t                        time elapsed: 0:22:07.942811\n",
            "====================================================================================================\n",
            "Epoch [7/84](12.50%)\t                            valid loss: 3.2453\t                            valid acc: 19.5312%\t                            time elapsed: 0:22:08.644384\n",
            "Epoch [7/84](100.00%)\t                            valid loss: 3.0748\t                            valid acc: 21.5622%\t                            time elapsed: 0:22:12.929717\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [8] Learning Rate : 0.0033\n",
            "Epoch [8/84](0.74%)\t                        train loss: 2.6284\t                        train acc: 35.1562%\t                        time elapsed: 0:22:14.455407\n",
            "Epoch [8/84](37.78%)\t                        train loss: 2.6016\t                        train acc: 30.0245%\t                        time elapsed: 0:23:14.183586\n",
            "Epoch [8/84](74.81%)\t                        train loss: 2.6049\t                        train acc: 29.7958%\t                        time elapsed: 0:24:12.571530\n",
            "Epoch [8/84](100.00%)\t                        train loss: 2.5950\t                        train acc: 29.8361%\t                        time elapsed: 0:24:53.080697\n",
            "====================================================================================================\n",
            "Epoch [8/84](12.50%)\t                            valid loss: 3.0295\t                            valid acc: 26.5625%\t                            time elapsed: 0:24:53.780981\n",
            "Epoch [8/84](100.00%)\t                            valid loss: 2.8888\t                            valid acc: 25.4125%\t                            time elapsed: 0:24:58.085557\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [9] Learning Rate : 0.0033\n",
            "Epoch [9/84](0.74%)\t                        train loss: 2.5818\t                        train acc: 31.2500%\t                        time elapsed: 0:24:59.601462\n",
            "Epoch [9/84](37.78%)\t                        train loss: 2.4832\t                        train acc: 33.1955%\t                        time elapsed: 0:25:58.055315\n",
            "Epoch [9/84](74.81%)\t                        train loss: 2.4760\t                        train acc: 33.1451%\t                        time elapsed: 0:26:57.506700\n",
            "Epoch [9/84](100.00%)\t                        train loss: 2.4616\t                        train acc: 33.1017%\t                        time elapsed: 0:27:37.234977\n",
            "====================================================================================================\n",
            "Epoch [9/84](12.50%)\t                            valid loss: 2.8221\t                            valid acc: 22.6562%\t                            time elapsed: 0:27:37.933792\n",
            "Epoch [9/84](100.00%)\t                            valid loss: 2.6432\t                            valid acc: 26.8427%\t                            time elapsed: 0:27:42.251659\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [10] Learning Rate : 0.0033\n",
            "Epoch [10/84](0.74%)\t                        train loss: 2.3251\t                        train acc: 38.2812%\t                        time elapsed: 0:27:44.116392\n",
            "Epoch [10/84](37.78%)\t                        train loss: 2.3774\t                        train acc: 34.6967%\t                        time elapsed: 0:28:43.605869\n",
            "Epoch [10/84](74.81%)\t                        train loss: 2.3747\t                        train acc: 34.8236%\t                        time elapsed: 0:29:43.199898\n",
            "Epoch [10/84](100.00%)\t                        train loss: 2.3662\t                        train acc: 34.8677%\t                        time elapsed: 0:30:22.623402\n",
            "====================================================================================================\n",
            "Epoch [10/84](12.50%)\t                            valid loss: 2.9716\t                            valid acc: 23.4375%\t                            time elapsed: 0:30:23.315835\n",
            "Epoch [10/84](100.00%)\t                            valid loss: 2.7074\t                            valid acc: 26.4026%\t                            time elapsed: 0:30:27.585203\n",
            "====================================================================================================\n",
            "Epoch [11] Learning Rate : 0.0033\n",
            "Epoch [11/84](0.74%)\t                        train loss: 2.1560\t                        train acc: 43.7500%\t                        time elapsed: 0:30:28.902878\n",
            "Epoch [11/84](37.78%)\t                        train loss: 2.2302\t                        train acc: 38.0362%\t                        time elapsed: 0:31:28.353225\n",
            "Epoch [11/84](74.81%)\t                        train loss: 2.2572\t                        train acc: 37.4845%\t                        time elapsed: 0:32:25.856582\n",
            "Epoch [11/84](100.00%)\t                        train loss: 2.2686\t                        train acc: 37.2242%\t                        time elapsed: 0:33:06.153926\n",
            "====================================================================================================\n",
            "Epoch [11/84](12.50%)\t                            valid loss: 2.4087\t                            valid acc: 28.9062%\t                            time elapsed: 0:33:06.855282\n",
            "Epoch [11/84](100.00%)\t                            valid loss: 2.4234\t                            valid acc: 34.2134%\t                            time elapsed: 0:33:11.157218\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [12] Learning Rate : 0.0033\n",
            "Epoch [12/84](0.74%)\t                        train loss: 2.4392\t                        train acc: 34.3750%\t                        time elapsed: 0:33:12.666412\n",
            "Epoch [12/84](37.78%)\t                        train loss: 2.1685\t                        train acc: 38.9400%\t                        time elapsed: 0:34:11.930197\n",
            "Epoch [12/84](74.81%)\t                        train loss: 2.1978\t                        train acc: 38.4514%\t                        time elapsed: 0:35:10.269029\n",
            "Epoch [12/84](100.00%)\t                        train loss: 2.1802\t                        train acc: 38.9786%\t                        time elapsed: 0:35:50.937952\n",
            "====================================================================================================\n",
            "Epoch [12/84](12.50%)\t                            valid loss: 2.2858\t                            valid acc: 32.0312%\t                            time elapsed: 0:35:51.637067\n",
            "Epoch [12/84](100.00%)\t                            valid loss: 2.1957\t                            valid acc: 37.8438%\t                            time elapsed: 0:35:55.926471\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [13] Learning Rate : 0.0033\n",
            "Epoch [13/84](0.74%)\t                        train loss: 2.0254\t                        train acc: 47.6562%\t                        time elapsed: 0:35:57.465820\n",
            "Epoch [13/84](37.78%)\t                        train loss: 2.1283\t                        train acc: 40.6403%\t                        time elapsed: 0:36:55.947544\n",
            "Epoch [13/84](74.81%)\t                        train loss: 2.1110\t                        train acc: 40.6095%\t                        time elapsed: 0:37:55.020937\n",
            "Epoch [13/84](100.00%)\t                        train loss: 2.1081\t                        train acc: 40.7214%\t                        time elapsed: 0:38:34.421716\n",
            "====================================================================================================\n",
            "Epoch [13/84](12.50%)\t                            valid loss: 2.0953\t                            valid acc: 37.5000%\t                            time elapsed: 0:38:35.126187\n",
            "Epoch [13/84](100.00%)\t                            valid loss: 2.1238\t                            valid acc: 37.9538%\t                            time elapsed: 0:38:39.387907\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [14] Learning Rate : 0.0033\n",
            "Epoch [14/84](0.74%)\t                        train loss: 1.9371\t                        train acc: 45.3125%\t                        time elapsed: 0:38:40.927571\n",
            "Epoch [14/84](37.78%)\t                        train loss: 2.0062\t                        train acc: 43.1066%\t                        time elapsed: 0:39:40.014858\n",
            "Epoch [14/84](74.81%)\t                        train loss: 2.0264\t                        train acc: 42.5201%\t                        time elapsed: 0:40:38.830445\n",
            "Epoch [14/84](100.00%)\t                        train loss: 2.0273\t                        train acc: 42.2963%\t                        time elapsed: 0:41:18.134468\n",
            "====================================================================================================\n",
            "Epoch [14/84](12.50%)\t                            valid loss: 2.3725\t                            valid acc: 33.5938%\t                            time elapsed: 0:41:18.830654\n",
            "Epoch [14/84](100.00%)\t                            valid loss: 2.3175\t                            valid acc: 35.7536%\t                            time elapsed: 0:41:23.078560\n",
            "====================================================================================================\n",
            "Epoch [15] Learning Rate : 0.0033\n",
            "Epoch [15/84](0.74%)\t                        train loss: 2.0906\t                        train acc: 39.8438%\t                        time elapsed: 0:41:24.360364\n",
            "Epoch [15/84](37.78%)\t                        train loss: 1.9542\t                        train acc: 43.7960%\t                        time elapsed: 0:42:23.435172\n",
            "Epoch [15/84](74.81%)\t                        train loss: 1.9776\t                        train acc: 42.8605%\t                        time elapsed: 0:43:21.370222\n",
            "Epoch [15/84](100.00%)\t                        train loss: 1.9804\t                        train acc: 43.0722%\t                        time elapsed: 0:44:01.632879\n",
            "====================================================================================================\n",
            "Epoch [15/84](12.50%)\t                            valid loss: 2.4084\t                            valid acc: 30.4688%\t                            time elapsed: 0:44:02.330460\n",
            "Epoch [15/84](100.00%)\t                            valid loss: 2.2067\t                            valid acc: 37.7338%\t                            time elapsed: 0:44:06.546642\n",
            "====================================================================================================\n",
            "Epoch [16] Learning Rate : 0.0033\n",
            "Epoch [16/84](0.74%)\t                        train loss: 1.8316\t                        train acc: 42.1875%\t                        time elapsed: 0:44:07.791521\n",
            "Epoch [16/84](37.78%)\t                        train loss: 1.8514\t                        train acc: 46.4767%\t                        time elapsed: 0:45:05.791747\n",
            "Epoch [16/84](74.81%)\t                        train loss: 1.8894\t                        train acc: 45.5291%\t                        time elapsed: 0:46:04.818231\n",
            "Epoch [16/84](100.00%)\t                        train loss: 1.8977\t                        train acc: 45.3535%\t                        time elapsed: 0:46:44.375649\n",
            "====================================================================================================\n",
            "Epoch [16/84](12.50%)\t                            valid loss: 2.7810\t                            valid acc: 31.2500%\t                            time elapsed: 0:46:45.061107\n",
            "Epoch [16/84](100.00%)\t                            valid loss: 2.6197\t                            valid acc: 33.0033%\t                            time elapsed: 0:46:49.303045\n",
            "====================================================================================================\n",
            "Epoch [17] Learning Rate : 0.0033\n",
            "Epoch [17/84](0.74%)\t                        train loss: 1.8733\t                        train acc: 40.6250%\t                        time elapsed: 0:46:50.572832\n",
            "Epoch [17/84](37.78%)\t                        train loss: 1.8413\t                        train acc: 46.7371%\t                        time elapsed: 0:47:49.590683\n",
            "Epoch [17/84](74.81%)\t                        train loss: 1.8656\t                        train acc: 46.4264%\t                        time elapsed: 0:48:48.296088\n",
            "Epoch [17/84](100.00%)\t                        train loss: 1.8666\t                        train acc: 46.4825%\t                        time elapsed: 0:49:27.824800\n",
            "====================================================================================================\n",
            "Epoch [17/84](12.50%)\t                            valid loss: 2.2070\t                            valid acc: 36.7188%\t                            time elapsed: 0:49:28.521047\n",
            "Epoch [17/84](100.00%)\t                            valid loss: 2.0317\t                            valid acc: 42.2442%\t                            time elapsed: 0:49:32.796265\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [18] Learning Rate : 0.0033\n",
            "Epoch [18/84](0.74%)\t                        train loss: 1.7708\t                        train acc: 48.4375%\t                        time elapsed: 0:49:34.327332\n",
            "Epoch [18/84](37.78%)\t                        train loss: 1.7770\t                        train acc: 48.1311%\t                        time elapsed: 0:50:33.324813\n",
            "Epoch [18/84](74.81%)\t                        train loss: 1.8089\t                        train acc: 47.5015%\t                        time elapsed: 0:51:32.124989\n",
            "Epoch [18/84](100.00%)\t                        train loss: 1.7997\t                        train acc: 47.6406%\t                        time elapsed: 0:52:12.167574\n",
            "====================================================================================================\n",
            "Epoch [18/84](12.50%)\t                            valid loss: 2.1087\t                            valid acc: 43.7500%\t                            time elapsed: 0:52:12.848470\n",
            "Epoch [18/84](100.00%)\t                            valid loss: 1.9932\t                            valid acc: 44.2244%\t                            time elapsed: 0:52:17.101547\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [19] Learning Rate : 0.0033\n",
            "Epoch [19/84](0.74%)\t                        train loss: 1.7605\t                        train acc: 49.2188%\t                        time elapsed: 0:52:18.676830\n",
            "Epoch [19/84](37.78%)\t                        train loss: 1.7302\t                        train acc: 49.6783%\t                        time elapsed: 0:53:16.805230\n",
            "Epoch [19/84](74.81%)\t                        train loss: 1.7600\t                        train acc: 48.9790%\t                        time elapsed: 0:54:15.596692\n",
            "Epoch [19/84](100.00%)\t                        train loss: 1.7665\t                        train acc: 48.9259%\t                        time elapsed: 0:54:54.963783\n",
            "====================================================================================================\n",
            "Epoch [19/84](12.50%)\t                            valid loss: 2.2858\t                            valid acc: 35.9375%\t                            time elapsed: 0:54:55.666524\n",
            "Epoch [19/84](100.00%)\t                            valid loss: 2.1994\t                            valid acc: 39.4939%\t                            time elapsed: 0:54:59.938020\n",
            "====================================================================================================\n",
            "Epoch [20] Learning Rate : 0.0033\n",
            "Epoch [20/84](0.74%)\t                        train loss: 1.8567\t                        train acc: 42.9688%\t                        time elapsed: 0:55:01.280554\n",
            "Epoch [20/84](37.78%)\t                        train loss: 1.6640\t                        train acc: 50.9191%\t                        time elapsed: 0:56:00.112629\n",
            "Epoch [20/84](74.81%)\t                        train loss: 1.7160\t                        train acc: 49.5900%\t                        time elapsed: 0:56:59.124028\n",
            "Epoch [20/84](100.00%)\t                        train loss: 1.7230\t                        train acc: 49.5281%\t                        time elapsed: 0:57:38.435819\n",
            "====================================================================================================\n",
            "Epoch [20/84](12.50%)\t                            valid loss: 1.8123\t                            valid acc: 45.3125%\t                            time elapsed: 0:57:39.140790\n",
            "Epoch [20/84](100.00%)\t                            valid loss: 1.7567\t                            valid acc: 47.5248%\t                            time elapsed: 0:57:43.384035\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [21] Learning Rate : 0.0033\n",
            "Epoch [21/84](0.74%)\t                        train loss: 1.7111\t                        train acc: 55.4688%\t                        time elapsed: 0:57:44.933408\n",
            "Epoch [21/84](37.78%)\t                        train loss: 1.6460\t                        train acc: 52.2978%\t                        time elapsed: 0:58:43.893873\n",
            "Epoch [21/84](74.81%)\t                        train loss: 1.6705\t                        train acc: 51.1216%\t                        time elapsed: 0:59:42.023636\n",
            "Epoch [21/84](100.00%)\t                        train loss: 1.6832\t                        train acc: 50.8193%\t                        time elapsed: 1:00:22.398837\n",
            "====================================================================================================\n",
            "Epoch [21/84](12.50%)\t                            valid loss: 1.9475\t                            valid acc: 45.3125%\t                            time elapsed: 1:00:23.085678\n",
            "Epoch [21/84](100.00%)\t                            valid loss: 1.7885\t                            valid acc: 47.8548%\t                            time elapsed: 1:00:27.304511\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [22] Learning Rate : 0.0033\n",
            "Epoch [22/84](0.74%)\t                        train loss: 1.6143\t                        train acc: 49.2188%\t                        time elapsed: 1:00:28.848028\n",
            "Epoch [22/84](37.78%)\t                        train loss: 1.5959\t                        train acc: 53.1710%\t                        time elapsed: 1:01:26.553976\n",
            "Epoch [22/84](74.81%)\t                        train loss: 1.6095\t                        train acc: 52.6532%\t                        time elapsed: 1:02:25.561916\n",
            "Epoch [22/84](100.00%)\t                        train loss: 1.6284\t                        train acc: 52.2321%\t                        time elapsed: 1:03:05.104839\n",
            "====================================================================================================\n",
            "Epoch [22/84](12.50%)\t                            valid loss: 1.9066\t                            valid acc: 50.0000%\t                            time elapsed: 1:03:05.794605\n",
            "Epoch [22/84](100.00%)\t                            valid loss: 1.9125\t                            valid acc: 44.3344%\t                            time elapsed: 1:03:10.021987\n",
            "====================================================================================================\n",
            "Epoch [23] Learning Rate : 0.0033\n",
            "Epoch [23/84](0.74%)\t                        train loss: 1.3838\t                        train acc: 60.9375%\t                        time elapsed: 1:03:11.314673\n",
            "Epoch [23/84](37.78%)\t                        train loss: 1.5571\t                        train acc: 53.5692%\t                        time elapsed: 1:04:10.095033\n",
            "Epoch [23/84](74.81%)\t                        train loss: 1.6148\t                        train acc: 52.5526%\t                        time elapsed: 1:05:09.070797\n",
            "Epoch [23/84](100.00%)\t                        train loss: 1.6143\t                        train acc: 52.5621%\t                        time elapsed: 1:05:48.527819\n",
            "====================================================================================================\n",
            "Epoch [23/84](12.50%)\t                            valid loss: 1.7368\t                            valid acc: 51.5625%\t                            time elapsed: 1:05:49.234111\n",
            "Epoch [23/84](100.00%)\t                            valid loss: 1.7343\t                            valid acc: 49.1749%\t                            time elapsed: 1:05:53.445328\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [24] Learning Rate : 0.0033\n",
            "Epoch [24/84](0.74%)\t                        train loss: 1.5444\t                        train acc: 51.5625%\t                        time elapsed: 1:05:54.983017\n",
            "Epoch [24/84](37.78%)\t                        train loss: 1.5640\t                        train acc: 53.5846%\t                        time elapsed: 1:06:53.968552\n",
            "Epoch [24/84](74.81%)\t                        train loss: 1.5826\t                        train acc: 53.2874%\t                        time elapsed: 1:07:52.163289\n",
            "Epoch [24/84](100.00%)\t                        train loss: 1.5907\t                        train acc: 53.1758%\t                        time elapsed: 1:08:32.311831\n",
            "====================================================================================================\n",
            "Epoch [24/84](12.50%)\t                            valid loss: 1.9015\t                            valid acc: 42.1875%\t                            time elapsed: 1:08:33.010279\n",
            "Epoch [24/84](100.00%)\t                            valid loss: 1.8692\t                            valid acc: 45.8746%\t                            time elapsed: 1:08:37.267225\n",
            "====================================================================================================\n",
            "Epoch [25] Learning Rate : 0.0033\n",
            "Epoch [25/84](0.74%)\t                        train loss: 1.7242\t                        train acc: 48.4375%\t                        time elapsed: 1:08:38.536744\n",
            "Epoch [25/84](37.78%)\t                        train loss: 1.5317\t                        train acc: 54.6875%\t                        time elapsed: 1:09:36.395341\n",
            "Epoch [25/84](74.81%)\t                        train loss: 1.5476\t                        train acc: 54.0919%\t                        time elapsed: 1:10:35.134158\n",
            "Epoch [25/84](100.00%)\t                        train loss: 1.5626\t                        train acc: 53.7143%\t                        time elapsed: 1:11:14.491297\n",
            "====================================================================================================\n",
            "Epoch [25/84](12.50%)\t                            valid loss: 2.0306\t                            valid acc: 45.3125%\t                            time elapsed: 1:11:15.177720\n",
            "Epoch [25/84](100.00%)\t                            valid loss: 1.9330\t                            valid acc: 43.7844%\t                            time elapsed: 1:11:19.390230\n",
            "====================================================================================================\n",
            "Epoch [26] Learning Rate : 0.0033\n",
            "Epoch [26/84](0.74%)\t                        train loss: 1.5636\t                        train acc: 53.1250%\t                        time elapsed: 1:11:20.683549\n",
            "Epoch [26/84](37.78%)\t                        train loss: 1.5175\t                        train acc: 55.7598%\t                        time elapsed: 1:12:19.021970\n",
            "Epoch [26/84](74.81%)\t                        train loss: 1.5409\t                        train acc: 55.0743%\t                        time elapsed: 1:13:17.809144\n",
            "Epoch [26/84](100.00%)\t                        train loss: 1.5384\t                        train acc: 54.9418%\t                        time elapsed: 1:13:57.149220\n",
            "====================================================================================================\n",
            "Epoch [26/84](12.50%)\t                            valid loss: 1.8529\t                            valid acc: 47.6562%\t                            time elapsed: 1:13:57.846973\n",
            "Epoch [26/84](100.00%)\t                            valid loss: 1.6678\t                            valid acc: 49.8350%\t                            time elapsed: 1:14:02.066920\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [27] Learning Rate : 0.0033\n",
            "Epoch [27/84](0.74%)\t                        train loss: 1.3339\t                        train acc: 56.2500%\t                        time elapsed: 1:14:03.613736\n",
            "Epoch [27/84](37.78%)\t                        train loss: 1.4738\t                        train acc: 56.0815%\t                        time elapsed: 1:15:02.258508\n",
            "Epoch [27/84](74.81%)\t                        train loss: 1.5062\t                        train acc: 55.4920%\t                        time elapsed: 1:16:00.329151\n",
            "Epoch [27/84](100.00%)\t                        train loss: 1.5085\t                        train acc: 55.2950%\t                        time elapsed: 1:16:40.301072\n",
            "====================================================================================================\n",
            "Epoch [27/84](12.50%)\t                            valid loss: 1.6892\t                            valid acc: 50.7812%\t                            time elapsed: 1:16:40.992316\n",
            "Epoch [27/84](100.00%)\t                            valid loss: 1.6542\t                            valid acc: 51.5952%\t                            time elapsed: 1:16:45.218685\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [28] Learning Rate : 0.0033\n",
            "Epoch [28/84](0.74%)\t                        train loss: 1.5009\t                        train acc: 56.2500%\t                        time elapsed: 1:16:46.773168\n",
            "Epoch [28/84](37.78%)\t                        train loss: 1.4551\t                        train acc: 56.6942%\t                        time elapsed: 1:17:44.915830\n",
            "Epoch [28/84](74.81%)\t                        train loss: 1.4620\t                        train acc: 56.4124%\t                        time elapsed: 1:18:43.398099\n",
            "Epoch [28/84](100.00%)\t                        train loss: 1.4788\t                        train acc: 56.1404%\t                        time elapsed: 1:19:22.629236\n",
            "====================================================================================================\n",
            "Epoch [28/84](12.50%)\t                            valid loss: 1.6893\t                            valid acc: 50.0000%\t                            time elapsed: 1:19:23.333956\n",
            "Epoch [28/84](100.00%)\t                            valid loss: 1.6225\t                            valid acc: 51.5952%\t                            time elapsed: 1:19:27.505931\n",
            "====================================================================================================\n",
            "Epoch [29] Learning Rate : 0.0033\n",
            "Epoch [29/84](0.74%)\t                        train loss: 1.5488\t                        train acc: 55.4688%\t                        time elapsed: 1:19:28.792693\n",
            "Epoch [29/84](37.78%)\t                        train loss: 1.4535\t                        train acc: 56.9547%\t                        time elapsed: 1:20:27.231505\n",
            "Epoch [29/84](74.81%)\t                        train loss: 1.4798\t                        train acc: 56.1185%\t                        time elapsed: 1:21:25.607306\n",
            "Epoch [29/84](100.00%)\t                        train loss: 1.4780\t                        train acc: 55.9145%\t                        time elapsed: 1:22:05.457803\n",
            "====================================================================================================\n",
            "Epoch [29/84](12.50%)\t                            valid loss: 1.6441\t                            valid acc: 53.1250%\t                            time elapsed: 1:22:06.154773\n",
            "Epoch [29/84](100.00%)\t                            valid loss: 1.6474\t                            valid acc: 50.7151%\t                            time elapsed: 1:22:10.374236\n",
            "====================================================================================================\n",
            "Epoch [30] Learning Rate : 0.0033\n",
            "Epoch [30/84](0.74%)\t                        train loss: 1.5755\t                        train acc: 52.3438%\t                        time elapsed: 1:22:11.679458\n",
            "Epoch [30/84](37.78%)\t                        train loss: 1.4291\t                        train acc: 57.6900%\t                        time elapsed: 1:23:10.368982\n",
            "Epoch [30/84](74.81%)\t                        train loss: 1.4361\t                        train acc: 57.3793%\t                        time elapsed: 1:24:08.280204\n",
            "Epoch [30/84](100.00%)\t                        train loss: 1.4503\t                        train acc: 56.9104%\t                        time elapsed: 1:24:48.449565\n",
            "====================================================================================================\n",
            "Epoch [30/84](12.50%)\t                            valid loss: 1.8789\t                            valid acc: 45.3125%\t                            time elapsed: 1:24:49.139723\n",
            "Epoch [30/84](100.00%)\t                            valid loss: 1.6791\t                            valid acc: 51.9252%\t                            time elapsed: 1:24:53.375180\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [31] Learning Rate : 0.0033\n",
            "Epoch [31/84](0.74%)\t                        train loss: 1.5333\t                        train acc: 57.8125%\t                        time elapsed: 1:24:54.941340\n",
            "Epoch [31/84](37.78%)\t                        train loss: 1.3921\t                        train acc: 59.1146%\t                        time elapsed: 1:25:52.788505\n",
            "Epoch [31/84](74.81%)\t                        train loss: 1.4254\t                        train acc: 57.7661%\t                        time elapsed: 1:26:51.470357\n",
            "Epoch [31/84](100.00%)\t                        train loss: 1.4291\t                        train acc: 57.6573%\t                        time elapsed: 1:27:30.385656\n",
            "====================================================================================================\n",
            "Epoch [31/84](12.50%)\t                            valid loss: 1.7960\t                            valid acc: 46.8750%\t                            time elapsed: 1:27:31.091362\n",
            "Epoch [31/84](100.00%)\t                            valid loss: 1.7761\t                            valid acc: 49.2849%\t                            time elapsed: 1:27:35.372550\n",
            "====================================================================================================\n",
            "Epoch [32] Learning Rate : 0.0033\n",
            "Epoch [32/84](0.74%)\t                        train loss: 1.4137\t                        train acc: 58.5938%\t                        time elapsed: 1:27:36.643574\n",
            "Epoch [32/84](37.78%)\t                        train loss: 1.3570\t                        train acc: 58.6397%\t                        time elapsed: 1:28:35.299260\n",
            "Epoch [32/84](74.81%)\t                        train loss: 1.3860\t                        train acc: 58.3540%\t                        time elapsed: 1:29:33.049157\n",
            "Epoch [32/84](100.00%)\t                        train loss: 1.4055\t                        train acc: 57.9237%\t                        time elapsed: 1:30:13.439876\n",
            "====================================================================================================\n",
            "Epoch [32/84](12.50%)\t                            valid loss: 1.6477\t                            valid acc: 52.3438%\t                            time elapsed: 1:30:14.126785\n",
            "Epoch [32/84](100.00%)\t                            valid loss: 1.5825\t                            valid acc: 53.0253%\t                            time elapsed: 1:30:18.314762\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [33] Learning Rate : 0.0033\n",
            "Epoch [33/84](0.74%)\t                        train loss: 1.3121\t                        train acc: 61.7188%\t                        time elapsed: 1:30:19.866849\n",
            "Epoch [33/84](37.78%)\t                        train loss: 1.3672\t                        train acc: 58.7929%\t                        time elapsed: 1:31:18.744160\n",
            "Epoch [33/84](74.81%)\t                        train loss: 1.3575\t                        train acc: 59.2899%\t                        time elapsed: 1:32:16.612303\n",
            "Epoch [33/84](100.00%)\t                        train loss: 1.3829\t                        train acc: 58.7285%\t                        time elapsed: 1:32:56.666379\n",
            "====================================================================================================\n",
            "Epoch [33/84](12.50%)\t                            valid loss: 1.8074\t                            valid acc: 47.6562%\t                            time elapsed: 1:32:57.370324\n",
            "Epoch [33/84](100.00%)\t                            valid loss: 1.6416\t                            valid acc: 50.3850%\t                            time elapsed: 1:33:01.631130\n",
            "====================================================================================================\n",
            "Epoch [34] Learning Rate : 0.0033\n",
            "Epoch [34/84](0.74%)\t                        train loss: 1.2784\t                        train acc: 67.9688%\t                        time elapsed: 1:33:02.921122\n",
            "Epoch [34/84](37.78%)\t                        train loss: 1.3430\t                        train acc: 59.8346%\t                        time elapsed: 1:34:00.931578\n",
            "Epoch [34/84](74.81%)\t                        train loss: 1.3720\t                        train acc: 59.0347%\t                        time elapsed: 1:34:59.708645\n",
            "Epoch [34/84](100.00%)\t                        train loss: 1.3777\t                        train acc: 58.9254%\t                        time elapsed: 1:35:38.815110\n",
            "====================================================================================================\n",
            "Epoch [34/84](12.50%)\t                            valid loss: 1.8142\t                            valid acc: 52.3438%\t                            time elapsed: 1:35:39.505552\n",
            "Epoch [34/84](100.00%)\t                            valid loss: 1.6829\t                            valid acc: 51.4851%\t                            time elapsed: 1:35:43.736632\n",
            "====================================================================================================\n",
            "Epoch [35] Learning Rate : 0.0033\n",
            "Epoch [35/84](0.74%)\t                        train loss: 1.5398\t                        train acc: 56.2500%\t                        time elapsed: 1:35:44.989131\n",
            "Epoch [35/84](37.78%)\t                        train loss: 1.3170\t                        train acc: 60.5699%\t                        time elapsed: 1:36:44.034809\n",
            "Epoch [35/84](74.81%)\t                        train loss: 1.3425\t                        train acc: 59.6767%\t                        time elapsed: 1:37:42.033164\n",
            "Epoch [35/84](100.00%)\t                        train loss: 1.3586\t                        train acc: 59.5333%\t                        time elapsed: 1:38:22.259014\n",
            "====================================================================================================\n",
            "Epoch [35/84](12.50%)\t                            valid loss: 1.5745\t                            valid acc: 55.4688%\t                            time elapsed: 1:38:22.952630\n",
            "Epoch [35/84](100.00%)\t                            valid loss: 1.5445\t                            valid acc: 53.2453%\t                            time elapsed: 1:38:27.177567\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [36] Learning Rate : 0.0033\n",
            "Epoch [36/84](0.74%)\t                        train loss: 1.3251\t                        train acc: 53.1250%\t                        time elapsed: 1:38:28.679826\n",
            "Epoch [36/84](37.78%)\t                        train loss: 1.3065\t                        train acc: 61.3358%\t                        time elapsed: 1:39:26.613125\n",
            "Epoch [36/84](74.81%)\t                        train loss: 1.3444\t                        train acc: 59.8159%\t                        time elapsed: 1:40:25.484823\n",
            "Epoch [36/84](100.00%)\t                        train loss: 1.3587\t                        train acc: 59.4754%\t                        time elapsed: 1:41:05.105909\n",
            "====================================================================================================\n",
            "Epoch [36/84](12.50%)\t                            valid loss: 1.6264\t                            valid acc: 57.8125%\t                            time elapsed: 1:41:05.778586\n",
            "Epoch [36/84](100.00%)\t                            valid loss: 1.4736\t                            valid acc: 56.8757%\t                            time elapsed: 1:41:11.221436\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [37] Learning Rate : 0.0033\n",
            "Epoch [37/84](0.74%)\t                        train loss: 1.2342\t                        train acc: 61.7188%\t                        time elapsed: 1:41:12.860763\n",
            "Epoch [37/84](37.78%)\t                        train loss: 1.3092\t                        train acc: 60.6618%\t                        time elapsed: 1:42:10.835269\n",
            "Epoch [37/84](74.81%)\t                        train loss: 1.3298\t                        train acc: 60.3032%\t                        time elapsed: 1:43:09.640664\n",
            "Epoch [37/84](100.00%)\t                        train loss: 1.3328\t                        train acc: 60.1702%\t                        time elapsed: 1:43:48.866531\n",
            "====================================================================================================\n",
            "Epoch [37/84](12.50%)\t                            valid loss: 1.5354\t                            valid acc: 56.2500%\t                            time elapsed: 1:43:49.557190\n",
            "Epoch [37/84](100.00%)\t                            valid loss: 1.4310\t                            valid acc: 58.5259%\t                            time elapsed: 1:43:53.807969\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [38] Learning Rate : 0.0033\n",
            "Epoch [38/84](0.74%)\t                        train loss: 1.1437\t                        train acc: 61.7188%\t                        time elapsed: 1:43:55.366991\n",
            "Epoch [38/84](37.78%)\t                        train loss: 1.2950\t                        train acc: 60.8609%\t                        time elapsed: 1:44:54.244141\n",
            "Epoch [38/84](74.81%)\t                        train loss: 1.3172\t                        train acc: 59.9706%\t                        time elapsed: 1:45:52.086858\n",
            "Epoch [38/84](100.00%)\t                        train loss: 1.3215\t                        train acc: 60.2802%\t                        time elapsed: 1:46:32.264666\n",
            "====================================================================================================\n",
            "Epoch [38/84](12.50%)\t                            valid loss: 1.6302\t                            valid acc: 50.7812%\t                            time elapsed: 1:46:32.971202\n",
            "Epoch [38/84](100.00%)\t                            valid loss: 1.5510\t                            valid acc: 54.1254%\t                            time elapsed: 1:46:37.213427\n",
            "====================================================================================================\n",
            "Epoch [39] Learning Rate : 0.0033\n",
            "Epoch [39/84](0.74%)\t                        train loss: 1.4116\t                        train acc: 60.1562%\t                        time elapsed: 1:46:38.465599\n",
            "Epoch [39/84](37.78%)\t                        train loss: 1.2872\t                        train acc: 60.6771%\t                        time elapsed: 1:47:36.346184\n",
            "Epoch [39/84](74.81%)\t                        train loss: 1.2961\t                        train acc: 60.7673%\t                        time elapsed: 1:48:35.475505\n",
            "Epoch [39/84](100.00%)\t                        train loss: 1.3039\t                        train acc: 60.4597%\t                        time elapsed: 1:49:14.701299\n",
            "====================================================================================================\n",
            "Epoch [39/84](12.50%)\t                            valid loss: 1.6470\t                            valid acc: 52.3438%\t                            time elapsed: 1:49:15.397610\n",
            "Epoch [39/84](100.00%)\t                            valid loss: 1.5396\t                            valid acc: 54.3454%\t                            time elapsed: 1:49:19.584169\n",
            "====================================================================================================\n",
            "Epoch [40] Learning Rate : 0.0033\n",
            "Epoch [40/84](0.74%)\t                        train loss: 1.1525\t                        train acc: 64.8438%\t                        time elapsed: 1:49:20.831714\n",
            "Epoch [40/84](37.78%)\t                        train loss: 1.2470\t                        train acc: 62.3162%\t                        time elapsed: 1:50:19.998710\n",
            "Epoch [40/84](74.81%)\t                        train loss: 1.2819\t                        train acc: 61.2082%\t                        time elapsed: 1:51:18.636392\n",
            "Epoch [40/84](100.00%)\t                        train loss: 1.2859\t                        train acc: 61.3630%\t                        time elapsed: 1:51:57.952918\n",
            "====================================================================================================\n",
            "Epoch [40/84](12.50%)\t                            valid loss: 1.6556\t                            valid acc: 54.6875%\t                            time elapsed: 1:51:58.642254\n",
            "Epoch [40/84](100.00%)\t                            valid loss: 1.5176\t                            valid acc: 55.8856%\t                            time elapsed: 1:52:02.857573\n",
            "====================================================================================================\n",
            "Epoch [41] Learning Rate : 0.0033\n",
            "Epoch [41/84](0.74%)\t                        train loss: 1.2824\t                        train acc: 61.7188%\t                        time elapsed: 1:52:04.100628\n",
            "Epoch [41/84](37.78%)\t                        train loss: 1.2204\t                        train acc: 63.5263%\t                        time elapsed: 1:53:02.726801\n",
            "Epoch [41/84](74.81%)\t                        train loss: 1.2749\t                        train acc: 61.9508%\t                        time elapsed: 1:54:00.801737\n",
            "Epoch [41/84](100.00%)\t                        train loss: 1.2895\t                        train acc: 61.6235%\t                        time elapsed: 1:54:40.808064\n",
            "====================================================================================================\n",
            "Epoch [41/84](12.50%)\t                            valid loss: 1.5987\t                            valid acc: 58.5938%\t                            time elapsed: 1:54:41.493168\n",
            "Epoch [41/84](100.00%)\t                            valid loss: 1.4492\t                            valid acc: 58.0858%\t                            time elapsed: 1:54:45.698414\n",
            "====================================================================================================\n",
            "Epoch [42] Learning Rate : 0.0033\n",
            "Epoch [42/84](0.74%)\t                        train loss: 1.1519\t                        train acc: 62.5000%\t                        time elapsed: 1:54:46.945296\n",
            "Epoch [42/84](37.78%)\t                        train loss: 1.1973\t                        train acc: 63.5263%\t                        time elapsed: 1:55:44.619873\n",
            "Epoch [42/84](74.81%)\t                        train loss: 1.2348\t                        train acc: 62.2912%\t                        time elapsed: 1:56:43.585396\n",
            "Epoch [42/84](100.00%)\t                        train loss: 1.2511\t                        train acc: 61.9420%\t                        time elapsed: 1:57:23.002920\n",
            "====================================================================================================\n",
            "Epoch [42/84](12.50%)\t                            valid loss: 1.6465\t                            valid acc: 55.4688%\t                            time elapsed: 1:57:23.706209\n",
            "Epoch [42/84](100.00%)\t                            valid loss: 1.4791\t                            valid acc: 57.3157%\t                            time elapsed: 1:57:27.938862\n",
            "====================================================================================================\n",
            "Epoch [43] Learning Rate : 0.0033\n",
            "Epoch [43/84](0.74%)\t                        train loss: 1.2650\t                        train acc: 62.5000%\t                        time elapsed: 1:57:29.222017\n",
            "Epoch [43/84](37.78%)\t                        train loss: 1.2129\t                        train acc: 63.6336%\t                        time elapsed: 1:58:27.949625\n",
            "Epoch [43/84](74.81%)\t                        train loss: 1.2520\t                        train acc: 62.4149%\t                        time elapsed: 1:59:26.589750\n",
            "Epoch [43/84](100.00%)\t                        train loss: 1.2678\t                        train acc: 61.9478%\t                        time elapsed: 2:00:05.980497\n",
            "====================================================================================================\n",
            "Epoch [43/84](12.50%)\t                            valid loss: 1.8438\t                            valid acc: 50.0000%\t                            time elapsed: 2:00:06.656794\n",
            "Epoch [43/84](100.00%)\t                            valid loss: 1.6867\t                            valid acc: 51.8152%\t                            time elapsed: 2:00:10.851609\n",
            "====================================================================================================\n",
            "Epoch [44] Learning Rate : 0.0033\n",
            "Epoch [44/84](0.74%)\t                        train loss: 1.2002\t                        train acc: 64.0625%\t                        time elapsed: 2:00:12.203312\n",
            "Epoch [44/84](37.78%)\t                        train loss: 1.2052\t                        train acc: 63.1281%\t                        time elapsed: 2:01:11.006067\n",
            "Epoch [44/84](74.81%)\t                        train loss: 1.2348\t                        train acc: 62.8171%\t                        time elapsed: 2:02:08.662273\n",
            "Epoch [44/84](100.00%)\t                        train loss: 1.2435\t                        train acc: 62.5673%\t                        time elapsed: 2:02:48.944000\n",
            "====================================================================================================\n",
            "Epoch [44/84](12.50%)\t                            valid loss: 1.8670\t                            valid acc: 52.3438%\t                            time elapsed: 2:02:49.625525\n",
            "Epoch [44/84](100.00%)\t                            valid loss: 1.6285\t                            valid acc: 53.9054%\t                            time elapsed: 2:02:53.832565\n",
            "====================================================================================================\n",
            "Epoch 00045: reducing learning rate of group 0 to 1.6500e-03.\n",
            "Epoch [45] Learning Rate : 0.00165\n",
            "Epoch [45/84](0.74%)\t                        train loss: 1.2250\t                        train acc: 60.1562%\t                        time elapsed: 2:02:55.072605\n",
            "Epoch [45/84](37.78%)\t                        train loss: 1.0516\t                        train acc: 67.9841%\t                        time elapsed: 2:03:52.871959\n",
            "Epoch [45/84](74.81%)\t                        train loss: 1.0286\t                        train acc: 68.6185%\t                        time elapsed: 2:04:51.303833\n",
            "Epoch [45/84](100.00%)\t                        train loss: 1.0216\t                        train acc: 68.9306%\t                        time elapsed: 2:05:30.428014\n",
            "====================================================================================================\n",
            "Epoch [45/84](12.50%)\t                            valid loss: 1.3799\t                            valid acc: 60.9375%\t                            time elapsed: 2:05:31.115549\n",
            "Epoch [45/84](100.00%)\t                            valid loss: 1.1199\t                            valid acc: 66.5567%\t                            time elapsed: 2:05:35.329183\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [46] Learning Rate : 0.00165\n",
            "Epoch [46/84](0.74%)\t                        train loss: 0.8082\t                        train acc: 74.2188%\t                        time elapsed: 2:05:36.859742\n",
            "Epoch [46/84](37.78%)\t                        train loss: 0.9193\t                        train acc: 71.3848%\t                        time elapsed: 2:06:35.695063\n",
            "Epoch [46/84](74.81%)\t                        train loss: 0.9450\t                        train acc: 70.9700%\t                        time elapsed: 2:07:33.573592\n",
            "Epoch [46/84](100.00%)\t                        train loss: 0.9595\t                        train acc: 70.6676%\t                        time elapsed: 2:08:13.786964\n",
            "====================================================================================================\n",
            "Epoch [46/84](12.50%)\t                            valid loss: 1.3494\t                            valid acc: 60.1562%\t                            time elapsed: 2:08:14.469717\n",
            "Epoch [46/84](100.00%)\t                            valid loss: 1.1442\t                            valid acc: 66.3366%\t                            time elapsed: 2:08:18.686517\n",
            "====================================================================================================\n",
            "Epoch [47] Learning Rate : 0.00165\n",
            "Epoch [47/84](0.74%)\t                        train loss: 0.8845\t                        train acc: 69.5312%\t                        time elapsed: 2:08:19.951221\n",
            "Epoch [47/84](37.78%)\t                        train loss: 0.9041\t                        train acc: 72.4112%\t                        time elapsed: 2:09:17.988594\n",
            "Epoch [47/84](74.81%)\t                        train loss: 0.9326\t                        train acc: 71.6507%\t                        time elapsed: 2:10:16.279566\n",
            "Epoch [47/84](100.00%)\t                        train loss: 0.9448\t                        train acc: 71.3103%\t                        time elapsed: 2:10:55.626209\n",
            "====================================================================================================\n",
            "Epoch [47/84](12.50%)\t                            valid loss: 1.4871\t                            valid acc: 60.1562%\t                            time elapsed: 2:10:56.322124\n",
            "Epoch [47/84](100.00%)\t                            valid loss: 1.1819\t                            valid acc: 65.7866%\t                            time elapsed: 2:11:00.557920\n",
            "====================================================================================================\n",
            "Epoch [48] Learning Rate : 0.00165\n",
            "Epoch [48/84](0.74%)\t                        train loss: 1.0142\t                        train acc: 70.3125%\t                        time elapsed: 2:11:01.852823\n",
            "Epoch [48/84](37.78%)\t                        train loss: 0.9102\t                        train acc: 72.3346%\t                        time elapsed: 2:12:00.613278\n",
            "Epoch [48/84](74.81%)\t                        train loss: 0.9211\t                        train acc: 72.0916%\t                        time elapsed: 2:12:58.650271\n",
            "Epoch [48/84](100.00%)\t                        train loss: 0.9303\t                        train acc: 71.6345%\t                        time elapsed: 2:13:38.503974\n",
            "====================================================================================================\n",
            "Epoch [48/84](12.50%)\t                            valid loss: 1.5054\t                            valid acc: 54.6875%\t                            time elapsed: 2:13:39.191206\n",
            "Epoch [48/84](100.00%)\t                            valid loss: 1.1978\t                            valid acc: 64.0264%\t                            time elapsed: 2:13:43.437543\n",
            "====================================================================================================\n",
            "Epoch [49] Learning Rate : 0.00165\n",
            "Epoch [49/84](0.74%)\t                        train loss: 0.8209\t                        train acc: 78.1250%\t                        time elapsed: 2:13:44.697986\n",
            "Epoch [49/84](37.78%)\t                        train loss: 0.8857\t                        train acc: 72.8860%\t                        time elapsed: 2:14:43.321477\n",
            "Epoch [49/84](74.81%)\t                        train loss: 0.9148\t                        train acc: 71.7280%\t                        time elapsed: 2:15:41.593671\n",
            "Epoch [49/84](100.00%)\t                        train loss: 0.9327\t                        train acc: 71.2871%\t                        time elapsed: 2:16:20.881821\n",
            "====================================================================================================\n",
            "Epoch [49/84](12.50%)\t                            valid loss: 1.3476\t                            valid acc: 62.5000%\t                            time elapsed: 2:16:21.562039\n",
            "Epoch [49/84](100.00%)\t                            valid loss: 1.1950\t                            valid acc: 65.6766%\t                            time elapsed: 2:16:26.140903\n",
            "====================================================================================================\n",
            "Epoch [50] Learning Rate : 0.00165\n",
            "Epoch [50/84](0.74%)\t                        train loss: 0.7325\t                        train acc: 74.2188%\t                        time elapsed: 2:16:27.946949\n",
            "Epoch [50/84](37.78%)\t                        train loss: 0.8963\t                        train acc: 72.0895%\t                        time elapsed: 2:17:26.047309\n",
            "Epoch [50/84](74.81%)\t                        train loss: 0.9181\t                        train acc: 71.6584%\t                        time elapsed: 2:18:24.635536\n",
            "Epoch [50/84](100.00%)\t                        train loss: 0.9261\t                        train acc: 71.5245%\t                        time elapsed: 2:19:03.858122\n",
            "====================================================================================================\n",
            "Epoch [50/84](12.50%)\t                            valid loss: 1.3265\t                            valid acc: 60.1562%\t                            time elapsed: 2:19:04.533230\n",
            "Epoch [50/84](100.00%)\t                            valid loss: 1.1583\t                            valid acc: 66.1166%\t                            time elapsed: 2:19:08.714302\n",
            "====================================================================================================\n",
            "Epoch [51] Learning Rate : 0.00165\n",
            "Epoch [51/84](0.74%)\t                        train loss: 0.8478\t                        train acc: 71.0938%\t                        time elapsed: 2:19:09.964823\n",
            "Epoch [51/84](37.78%)\t                        train loss: 0.9002\t                        train acc: 72.3805%\t                        time elapsed: 2:20:08.188269\n",
            "Epoch [51/84](74.81%)\t                        train loss: 0.9089\t                        train acc: 71.9601%\t                        time elapsed: 2:21:05.414015\n",
            "Epoch [51/84](100.00%)\t                        train loss: 0.9176\t                        train acc: 71.4724%\t                        time elapsed: 2:21:45.348293\n",
            "====================================================================================================\n",
            "Epoch [51/84](12.50%)\t                            valid loss: 1.3627\t                            valid acc: 56.2500%\t                            time elapsed: 2:21:46.041529\n",
            "Epoch [51/84](100.00%)\t                            valid loss: 1.1841\t                            valid acc: 66.2266%\t                            time elapsed: 2:21:50.222381\n",
            "====================================================================================================\n",
            "Epoch [52] Learning Rate : 0.00165\n",
            "Epoch [52/84](0.74%)\t                        train loss: 0.7498\t                        train acc: 79.6875%\t                        time elapsed: 2:21:51.463023\n",
            "Epoch [52/84](37.78%)\t                        train loss: 0.9063\t                        train acc: 72.0435%\t                        time elapsed: 2:22:48.803060\n",
            "Epoch [52/84](74.81%)\t                        train loss: 0.9132\t                        train acc: 71.7744%\t                        time elapsed: 2:23:46.965498\n",
            "Epoch [52/84](100.00%)\t                        train loss: 0.9141\t                        train acc: 71.7503%\t                        time elapsed: 2:24:25.861955\n",
            "====================================================================================================\n",
            "Epoch [52/84](12.50%)\t                            valid loss: 1.2958\t                            valid acc: 65.6250%\t                            time elapsed: 2:24:26.552273\n",
            "Epoch [52/84](100.00%)\t                            valid loss: 1.1327\t                            valid acc: 66.2266%\t                            time elapsed: 2:24:30.711870\n",
            "====================================================================================================\n",
            "Epoch 00053: reducing learning rate of group 0 to 8.2500e-04.\n",
            "Epoch [53] Learning Rate : 0.000825\n",
            "Epoch [53/84](0.74%)\t                        train loss: 0.9646\t                        train acc: 72.6562%\t                        time elapsed: 2:24:31.970659\n",
            "Epoch [53/84](37.78%)\t                        train loss: 0.7695\t                        train acc: 76.4093%\t                        time elapsed: 2:25:30.401618\n",
            "Epoch [53/84](74.81%)\t                        train loss: 0.7694\t                        train acc: 76.3304%\t                        time elapsed: 2:26:27.568434\n",
            "Epoch [53/84](100.00%)\t                        train loss: 0.7782\t                        train acc: 76.2318%\t                        time elapsed: 2:27:07.242689\n",
            "====================================================================================================\n",
            "Epoch [53/84](12.50%)\t                            valid loss: 1.2342\t                            valid acc: 64.0625%\t                            time elapsed: 2:27:07.908961\n",
            "Epoch [53/84](100.00%)\t                            valid loss: 0.9919\t                            valid acc: 69.3069%\t                            time elapsed: 2:27:12.070795\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [54] Learning Rate : 0.000825\n",
            "Epoch [54/84](0.74%)\t                        train loss: 0.9268\t                        train acc: 71.8750%\t                        time elapsed: 2:27:13.636436\n",
            "Epoch [54/84](37.78%)\t                        train loss: 0.7272\t                        train acc: 77.5429%\t                        time elapsed: 2:28:10.852549\n",
            "Epoch [54/84](74.81%)\t                        train loss: 0.7398\t                        train acc: 77.0730%\t                        time elapsed: 2:29:09.210966\n",
            "Epoch [54/84](100.00%)\t                        train loss: 0.7329\t                        train acc: 77.4883%\t                        time elapsed: 2:29:47.920549\n",
            "====================================================================================================\n",
            "Epoch [54/84](12.50%)\t                            valid loss: 1.2434\t                            valid acc: 63.2812%\t                            time elapsed: 2:29:48.602910\n",
            "Epoch [54/84](100.00%)\t                            valid loss: 0.9741\t                            valid acc: 70.1870%\t                            time elapsed: 2:29:52.726484\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [55] Learning Rate : 0.000825\n",
            "Epoch [55/84](0.74%)\t                        train loss: 0.8388\t                        train acc: 80.4688%\t                        time elapsed: 2:29:54.273215\n",
            "Epoch [55/84](37.78%)\t                        train loss: 0.7151\t                        train acc: 78.5846%\t                        time elapsed: 2:30:52.546268\n",
            "Epoch [55/84](74.81%)\t                        train loss: 0.7119\t                        train acc: 78.4267%\t                        time elapsed: 2:31:49.677573\n",
            "Epoch [55/84](100.00%)\t                        train loss: 0.7200\t                        train acc: 78.0325%\t                        time elapsed: 2:32:29.147765\n",
            "====================================================================================================\n",
            "Epoch [55/84](12.50%)\t                            valid loss: 1.2173\t                            valid acc: 63.2812%\t                            time elapsed: 2:32:29.827304\n",
            "Epoch [55/84](100.00%)\t                            valid loss: 0.9929\t                            valid acc: 69.1969%\t                            time elapsed: 2:32:33.996863\n",
            "====================================================================================================\n",
            "Epoch [56] Learning Rate : 0.000825\n",
            "Epoch [56/84](0.74%)\t                        train loss: 0.6434\t                        train acc: 79.6875%\t                        time elapsed: 2:32:35.284684\n",
            "Epoch [56/84](37.78%)\t                        train loss: 0.6836\t                        train acc: 79.5496%\t                        time elapsed: 2:33:32.427731\n",
            "Epoch [56/84](74.81%)\t                        train loss: 0.7008\t                        train acc: 78.9295%\t                        time elapsed: 2:34:30.704745\n",
            "Epoch [56/84](100.00%)\t                        train loss: 0.7102\t                        train acc: 78.3220%\t                        time elapsed: 2:35:09.755970\n",
            "====================================================================================================\n",
            "Epoch [56/84](12.50%)\t                            valid loss: 1.2221\t                            valid acc: 65.6250%\t                            time elapsed: 2:35:10.434678\n",
            "Epoch [56/84](100.00%)\t                            valid loss: 1.0152\t                            valid acc: 70.9571%\t                            time elapsed: 2:35:14.601366\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [57] Learning Rate : 0.000825\n",
            "Epoch [57/84](0.74%)\t                        train loss: 0.6160\t                        train acc: 76.5625%\t                        time elapsed: 2:35:16.127666\n",
            "Epoch [57/84](37.78%)\t                        train loss: 0.6892\t                        train acc: 79.0135%\t                        time elapsed: 2:36:14.546731\n",
            "Epoch [57/84](74.81%)\t                        train loss: 0.7065\t                        train acc: 78.4499%\t                        time elapsed: 2:37:11.564192\n",
            "Epoch [57/84](100.00%)\t                        train loss: 0.7053\t                        train acc: 78.4436%\t                        time elapsed: 2:37:51.497587\n",
            "====================================================================================================\n",
            "Epoch [57/84](12.50%)\t                            valid loss: 1.2050\t                            valid acc: 62.5000%\t                            time elapsed: 2:37:52.189193\n",
            "Epoch [57/84](100.00%)\t                            valid loss: 1.0000\t                            valid acc: 69.0869%\t                            time elapsed: 2:37:56.324640\n",
            "====================================================================================================\n",
            "Epoch [58] Learning Rate : 0.000825\n",
            "Epoch [58/84](0.74%)\t                        train loss: 0.5420\t                        train acc: 82.8125%\t                        time elapsed: 2:37:57.548828\n",
            "Epoch [58/84](37.78%)\t                        train loss: 0.6768\t                        train acc: 79.2126%\t                        time elapsed: 2:38:54.980210\n",
            "Epoch [58/84](74.81%)\t                        train loss: 0.6901\t                        train acc: 78.4267%\t                        time elapsed: 2:39:53.264421\n",
            "Epoch [58/84](100.00%)\t                        train loss: 0.6949\t                        train acc: 78.4784%\t                        time elapsed: 2:40:32.294529\n",
            "====================================================================================================\n",
            "Epoch [58/84](12.50%)\t                            valid loss: 1.1860\t                            valid acc: 66.4062%\t                            time elapsed: 2:40:32.971151\n",
            "Epoch [58/84](100.00%)\t                            valid loss: 0.9942\t                            valid acc: 69.9670%\t                            time elapsed: 2:40:37.132595\n",
            "====================================================================================================\n",
            "Epoch [59] Learning Rate : 0.000825\n",
            "Epoch [59/84](0.74%)\t                        train loss: 0.6047\t                        train acc: 84.3750%\t                        time elapsed: 2:40:38.389871\n",
            "Epoch [59/84](37.78%)\t                        train loss: 0.6617\t                        train acc: 78.9522%\t                        time elapsed: 2:41:36.897383\n",
            "Epoch [59/84](74.81%)\t                        train loss: 0.6760\t                        train acc: 79.0300%\t                        time elapsed: 2:42:34.259333\n",
            "Epoch [59/84](100.00%)\t                        train loss: 0.6914\t                        train acc: 78.6637%\t                        time elapsed: 2:43:14.132034\n",
            "====================================================================================================\n",
            "Epoch [59/84](12.50%)\t                            valid loss: 1.1933\t                            valid acc: 65.6250%\t                            time elapsed: 2:43:14.801927\n",
            "Epoch [59/84](100.00%)\t                            valid loss: 1.0224\t                            valid acc: 69.9670%\t                            time elapsed: 2:43:18.901226\n",
            "====================================================================================================\n",
            "Epoch [60] Learning Rate : 0.000825\n",
            "Epoch [60/84](0.74%)\t                        train loss: 0.7200\t                        train acc: 78.9062%\t                        time elapsed: 2:43:20.157152\n",
            "Epoch [60/84](37.78%)\t                        train loss: 0.6517\t                        train acc: 80.0092%\t                        time elapsed: 2:44:17.621524\n",
            "Epoch [60/84](74.81%)\t                        train loss: 0.6620\t                        train acc: 79.5483%\t                        time elapsed: 2:45:15.765758\n",
            "Epoch [60/84](100.00%)\t                        train loss: 0.6750\t                        train acc: 79.0747%\t                        time elapsed: 2:45:54.668242\n",
            "====================================================================================================\n",
            "Epoch [60/84](12.50%)\t                            valid loss: 1.1061\t                            valid acc: 65.6250%\t                            time elapsed: 2:45:55.350303\n",
            "Epoch [60/84](100.00%)\t                            valid loss: 0.9938\t                            valid acc: 69.1969%\t                            time elapsed: 2:45:59.519489\n",
            "====================================================================================================\n",
            "Epoch [61] Learning Rate : 0.000825\n",
            "Epoch [61/84](0.74%)\t                        train loss: 0.6402\t                        train acc: 81.2500%\t                        time elapsed: 2:46:00.776988\n",
            "Epoch [61/84](37.78%)\t                        train loss: 0.6651\t                        train acc: 79.8254%\t                        time elapsed: 2:46:59.005469\n",
            "Epoch [61/84](74.81%)\t                        train loss: 0.6642\t                        train acc: 79.8809%\t                        time elapsed: 2:47:56.990311\n",
            "Epoch [61/84](100.00%)\t                        train loss: 0.6652\t                        train acc: 79.7174%\t                        time elapsed: 2:48:36.082922\n",
            "====================================================================================================\n",
            "Epoch [61/84](12.50%)\t                            valid loss: 1.2178\t                            valid acc: 70.3125%\t                            time elapsed: 2:48:36.766568\n",
            "Epoch [61/84](100.00%)\t                            valid loss: 1.0219\t                            valid acc: 70.2970%\t                            time elapsed: 2:48:40.904184\n",
            "====================================================================================================\n",
            "Epoch [62] Learning Rate : 0.000825\n",
            "Epoch [62/84](0.74%)\t                        train loss: 0.7234\t                        train acc: 78.1250%\t                        time elapsed: 2:48:42.144948\n",
            "Epoch [62/84](37.78%)\t                        train loss: 0.6382\t                        train acc: 80.3156%\t                        time elapsed: 2:49:39.745560\n",
            "Epoch [62/84](74.81%)\t                        train loss: 0.6607\t                        train acc: 79.5173%\t                        time elapsed: 2:50:38.052076\n",
            "Epoch [62/84](100.00%)\t                        train loss: 0.6679\t                        train acc: 79.1037%\t                        time elapsed: 2:51:16.867457\n",
            "====================================================================================================\n",
            "Epoch [62/84](12.50%)\t                            valid loss: 1.2781\t                            valid acc: 65.6250%\t                            time elapsed: 2:51:17.551941\n",
            "Epoch [62/84](100.00%)\t                            valid loss: 1.0446\t                            valid acc: 68.2068%\t                            time elapsed: 2:51:21.707874\n",
            "====================================================================================================\n",
            "Epoch [63] Learning Rate : 0.000825\n",
            "Epoch [63/84](0.74%)\t                        train loss: 0.6690\t                        train acc: 80.4688%\t                        time elapsed: 2:51:22.956474\n",
            "Epoch [63/84](37.78%)\t                        train loss: 0.6495\t                        train acc: 79.6875%\t                        time elapsed: 2:52:21.384762\n",
            "Epoch [63/84](74.81%)\t                        train loss: 0.6655\t                        train acc: 79.3239%\t                        time elapsed: 2:53:19.385621\n",
            "Epoch [63/84](100.00%)\t                        train loss: 0.6697\t                        train acc: 79.1384%\t                        time elapsed: 2:53:58.232041\n",
            "====================================================================================================\n",
            "Epoch [63/84](12.50%)\t                            valid loss: 1.2376\t                            valid acc: 62.5000%\t                            time elapsed: 2:53:58.914250\n",
            "Epoch [63/84](100.00%)\t                            valid loss: 1.0247\t                            valid acc: 67.4367%\t                            time elapsed: 2:54:03.115894\n",
            "====================================================================================================\n",
            "Epoch 00064: reducing learning rate of group 0 to 4.1250e-04.\n",
            "Epoch [64] Learning Rate : 0.0004125\n",
            "Epoch [64/84](0.74%)\t                        train loss: 0.5143\t                        train acc: 86.7188%\t                        time elapsed: 2:54:04.369310\n",
            "Epoch [64/84](37.78%)\t                        train loss: 0.5881\t                        train acc: 82.3529%\t                        time elapsed: 2:55:01.810546\n",
            "Epoch [64/84](74.81%)\t                        train loss: 0.5910\t                        train acc: 82.4103%\t                        time elapsed: 2:55:59.719266\n",
            "Epoch [64/84](100.00%)\t                        train loss: 0.5883\t                        train acc: 82.3751%\t                        time elapsed: 2:56:38.681073\n",
            "====================================================================================================\n",
            "Epoch [64/84](12.50%)\t                            valid loss: 1.1870\t                            valid acc: 68.7500%\t                            time elapsed: 2:56:39.360932\n",
            "Epoch [64/84](100.00%)\t                            valid loss: 0.9399\t                            valid acc: 71.1771%\t                            time elapsed: 2:56:43.545527\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [65] Learning Rate : 0.0004125\n",
            "Epoch [65/84](0.74%)\t                        train loss: 0.5206\t                        train acc: 84.3750%\t                        time elapsed: 2:56:45.039482\n",
            "Epoch [65/84](37.78%)\t                        train loss: 0.5398\t                        train acc: 84.2065%\t                        time elapsed: 2:57:43.450564\n",
            "Epoch [65/84](74.81%)\t                        train loss: 0.5552\t                        train acc: 83.4855%\t                        time elapsed: 2:58:41.555593\n",
            "Epoch [65/84](100.00%)\t                        train loss: 0.5599\t                        train acc: 83.2783%\t                        time elapsed: 2:59:20.443339\n",
            "====================================================================================================\n",
            "Epoch [65/84](12.50%)\t                            valid loss: 1.1951\t                            valid acc: 67.9688%\t                            time elapsed: 2:59:21.127827\n",
            "Epoch [65/84](100.00%)\t                            valid loss: 0.9540\t                            valid acc: 72.7173%\t                            time elapsed: 2:59:25.264725\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [66] Learning Rate : 0.0004125\n",
            "Epoch [66/84](0.74%)\t                        train loss: 0.5637\t                        train acc: 81.2500%\t                        time elapsed: 2:59:26.760114\n",
            "Epoch [66/84](37.78%)\t                        train loss: 0.5363\t                        train acc: 83.9308%\t                        time elapsed: 3:00:23.979560\n",
            "Epoch [66/84](74.81%)\t                        train loss: 0.5444\t                        train acc: 83.6556%\t                        time elapsed: 3:01:22.385200\n",
            "Epoch [66/84](100.00%)\t                        train loss: 0.5486\t                        train acc: 83.4636%\t                        time elapsed: 3:02:01.262154\n",
            "====================================================================================================\n",
            "Epoch [66/84](12.50%)\t                            valid loss: 1.1873\t                            valid acc: 64.8438%\t                            time elapsed: 3:02:01.941806\n",
            "Epoch [66/84](100.00%)\t                            valid loss: 0.9579\t                            valid acc: 71.6172%\t                            time elapsed: 3:02:06.113050\n",
            "====================================================================================================\n",
            "Epoch [67] Learning Rate : 0.0004125\n",
            "Epoch [67/84](0.74%)\t                        train loss: 0.5348\t                        train acc: 81.2500%\t                        time elapsed: 3:02:07.373927\n",
            "Epoch [67/84](37.78%)\t                        train loss: 0.5362\t                        train acc: 83.7623%\t                        time elapsed: 3:03:05.877505\n",
            "Epoch [67/84](74.81%)\t                        train loss: 0.5451\t                        train acc: 83.6943%\t                        time elapsed: 3:04:03.443533\n",
            "Epoch [67/84](100.00%)\t                        train loss: 0.5492\t                        train acc: 83.5852%\t                        time elapsed: 3:04:43.714800\n",
            "====================================================================================================\n",
            "Epoch [67/84](12.50%)\t                            valid loss: 1.1933\t                            valid acc: 66.4062%\t                            time elapsed: 3:04:44.393894\n",
            "Epoch [67/84](100.00%)\t                            valid loss: 0.9464\t                            valid acc: 72.2772%\t                            time elapsed: 3:04:48.614564\n",
            "====================================================================================================\n",
            "Epoch [68] Learning Rate : 0.0004125\n",
            "Epoch [68/84](0.74%)\t                        train loss: 0.4569\t                        train acc: 87.5000%\t                        time elapsed: 3:04:49.865769\n",
            "Epoch [68/84](37.78%)\t                        train loss: 0.5156\t                        train acc: 84.8805%\t                        time elapsed: 3:05:47.883063\n",
            "Epoch [68/84](74.81%)\t                        train loss: 0.5280\t                        train acc: 84.3750%\t                        time elapsed: 3:06:46.777062\n",
            "Epoch [68/84](100.00%)\t                        train loss: 0.5319\t                        train acc: 84.2279%\t                        time elapsed: 3:07:25.809659\n",
            "====================================================================================================\n",
            "Epoch [68/84](12.50%)\t                            valid loss: 1.1938\t                            valid acc: 60.9375%\t                            time elapsed: 3:07:26.492653\n",
            "Epoch [68/84](100.00%)\t                            valid loss: 0.9408\t                            valid acc: 69.6370%\t                            time elapsed: 3:07:30.688873\n",
            "====================================================================================================\n",
            "Epoch [69] Learning Rate : 0.0004125\n",
            "Epoch [69/84](0.74%)\t                        train loss: 0.3991\t                        train acc: 86.7188%\t                        time elapsed: 3:07:31.952458\n",
            "Epoch [69/84](37.78%)\t                        train loss: 0.5104\t                        train acc: 84.6507%\t                        time elapsed: 3:08:30.385843\n",
            "Epoch [69/84](74.81%)\t                        train loss: 0.5196\t                        train acc: 84.4059%\t                        time elapsed: 3:09:28.642792\n",
            "Epoch [69/84](100.00%)\t                        train loss: 0.5254\t                        train acc: 84.1295%\t                        time elapsed: 3:10:08.467595\n",
            "====================================================================================================\n",
            "Epoch [69/84](12.50%)\t                            valid loss: 1.1538\t                            valid acc: 67.1875%\t                            time elapsed: 3:10:09.144795\n",
            "Epoch [69/84](100.00%)\t                            valid loss: 0.9536\t                            valid acc: 71.6172%\t                            time elapsed: 3:10:13.347022\n",
            "====================================================================================================\n",
            "Epoch [70] Learning Rate : 0.0004125\n",
            "Epoch [70/84](0.74%)\t                        train loss: 0.6476\t                        train acc: 77.3438%\t                        time elapsed: 3:10:14.668339\n",
            "Epoch [70/84](37.78%)\t                        train loss: 0.5251\t                        train acc: 84.1605%\t                        time elapsed: 3:11:11.992364\n",
            "Epoch [70/84](74.81%)\t                        train loss: 0.5223\t                        train acc: 84.4369%\t                        time elapsed: 3:12:10.491193\n",
            "Epoch [70/84](100.00%)\t                        train loss: 0.5214\t                        train acc: 84.5058%\t                        time elapsed: 3:12:49.397042\n",
            "====================================================================================================\n",
            "Epoch [70/84](12.50%)\t                            valid loss: 1.1679\t                            valid acc: 66.4062%\t                            time elapsed: 3:12:50.075927\n",
            "Epoch [70/84](100.00%)\t                            valid loss: 0.9402\t                            valid acc: 71.3971%\t                            time elapsed: 3:12:54.220793\n",
            "====================================================================================================\n",
            "Epoch [71] Learning Rate : 0.0004125\n",
            "Epoch [71/84](0.74%)\t                        train loss: 0.6601\t                        train acc: 78.1250%\t                        time elapsed: 3:12:55.474658\n",
            "Epoch [71/84](37.78%)\t                        train loss: 0.5113\t                        train acc: 85.0643%\t                        time elapsed: 3:13:54.036951\n",
            "Epoch [71/84](74.81%)\t                        train loss: 0.5121\t                        train acc: 84.9938%\t                        time elapsed: 3:14:51.579283\n",
            "Epoch [71/84](100.00%)\t                        train loss: 0.5199\t                        train acc: 84.4827%\t                        time elapsed: 3:15:31.449614\n",
            "====================================================================================================\n",
            "Epoch [71/84](12.50%)\t                            valid loss: 1.0779\t                            valid acc: 67.9688%\t                            time elapsed: 3:15:32.128504\n",
            "Epoch [71/84](100.00%)\t                            valid loss: 0.9576\t                            valid acc: 71.3971%\t                            time elapsed: 3:15:36.288201\n",
            "====================================================================================================\n",
            "Epoch [72] Learning Rate : 0.0004125\n",
            "Epoch [72/84](0.74%)\t                        train loss: 0.5119\t                        train acc: 89.0625%\t                        time elapsed: 3:15:37.522200\n",
            "Epoch [72/84](37.78%)\t                        train loss: 0.5032\t                        train acc: 85.2941%\t                        time elapsed: 3:16:35.106881\n",
            "Epoch [72/84](74.81%)\t                        train loss: 0.5101\t                        train acc: 85.0325%\t                        time elapsed: 3:17:32.937008\n",
            "Epoch [72/84](100.00%)\t                        train loss: 0.5133\t                        train acc: 84.8301%\t                        time elapsed: 3:18:11.926210\n",
            "====================================================================================================\n",
            "Epoch [72/84](12.50%)\t                            valid loss: 1.1708\t                            valid acc: 69.5312%\t                            time elapsed: 3:18:12.612735\n",
            "Epoch [72/84](100.00%)\t                            valid loss: 0.9621\t                            valid acc: 71.6172%\t                            time elapsed: 3:18:16.793342\n",
            "====================================================================================================\n",
            "Epoch 00073: reducing learning rate of group 0 to 2.0625e-04.\n",
            "Epoch [73] Learning Rate : 0.00020625\n",
            "Epoch [73/84](0.74%)\t                        train loss: 0.4632\t                        train acc: 85.9375%\t                        time elapsed: 3:18:18.035355\n",
            "Epoch [73/84](37.78%)\t                        train loss: 0.4792\t                        train acc: 85.5392%\t                        time elapsed: 3:19:16.382852\n",
            "Epoch [73/84](74.81%)\t                        train loss: 0.4807\t                        train acc: 85.6049%\t                        time elapsed: 3:20:16.215212\n",
            "Epoch [73/84](100.00%)\t                        train loss: 0.4787\t                        train acc: 85.7738%\t                        time elapsed: 3:20:57.357750\n",
            "====================================================================================================\n",
            "Epoch [73/84](12.50%)\t                            valid loss: 1.1525\t                            valid acc: 65.6250%\t                            time elapsed: 3:20:58.039076\n",
            "Epoch [73/84](100.00%)\t                            valid loss: 0.9294\t                            valid acc: 72.0572%\t                            time elapsed: 3:21:02.211980\n",
            "====================================================================================================\n",
            "Epoch [74] Learning Rate : 0.00020625\n",
            "Epoch [74/84](0.74%)\t                        train loss: 0.3628\t                        train acc: 88.2812%\t                        time elapsed: 3:21:03.450602\n",
            "Epoch [74/84](37.78%)\t                        train loss: 0.4737\t                        train acc: 86.0447%\t                        time elapsed: 3:22:00.735860\n",
            "Epoch [74/84](74.81%)\t                        train loss: 0.4676\t                        train acc: 85.9916%\t                        time elapsed: 3:22:59.061617\n",
            "Epoch [74/84](100.00%)\t                        train loss: 0.4671\t                        train acc: 86.1965%\t                        time elapsed: 3:23:38.001962\n",
            "====================================================================================================\n",
            "Epoch [74/84](12.50%)\t                            valid loss: 1.1332\t                            valid acc: 70.3125%\t                            time elapsed: 3:23:38.677429\n",
            "Epoch [74/84](100.00%)\t                            valid loss: 0.9236\t                            valid acc: 72.2772%\t                            time elapsed: 3:23:42.839911\n",
            "====================================================================================================\n",
            "Epoch [75] Learning Rate : 0.00020625\n",
            "Epoch [75/84](0.74%)\t                        train loss: 0.4251\t                        train acc: 89.0625%\t                        time elapsed: 3:23:44.086638\n",
            "Epoch [75/84](37.78%)\t                        train loss: 0.4427\t                        train acc: 87.6072%\t                        time elapsed: 3:24:42.213676\n",
            "Epoch [75/84](74.81%)\t                        train loss: 0.4517\t                        train acc: 87.0668%\t                        time elapsed: 3:25:39.947790\n",
            "Epoch [75/84](100.00%)\t                        train loss: 0.4563\t                        train acc: 86.8392%\t                        time elapsed: 3:26:18.866702\n",
            "====================================================================================================\n",
            "Epoch [75/84](12.50%)\t                            valid loss: 1.1177\t                            valid acc: 71.0938%\t                            time elapsed: 3:26:19.540436\n",
            "Epoch [75/84](100.00%)\t                            valid loss: 0.9171\t                            valid acc: 73.5974%\t                            time elapsed: 3:26:23.699103\n",
            "Best weight saved!!\n",
            "====================================================================================================\n",
            "Epoch [76] Learning Rate : 0.00020625\n",
            "Epoch [76/84](0.74%)\t                        train loss: 0.3303\t                        train acc: 92.1875%\t                        time elapsed: 3:26:25.251036\n",
            "Epoch [76/84](37.78%)\t                        train loss: 0.4301\t                        train acc: 87.7757%\t                        time elapsed: 3:27:23.747571\n",
            "Epoch [76/84](74.81%)\t                        train loss: 0.4386\t                        train acc: 87.3298%\t                        time elapsed: 3:28:21.180747\n",
            "Epoch [76/84](100.00%)\t                        train loss: 0.4489\t                        train acc: 87.0245%\t                        time elapsed: 3:29:00.686123\n",
            "====================================================================================================\n",
            "Epoch [76/84](12.50%)\t                            valid loss: 1.1771\t                            valid acc: 64.8438%\t                            time elapsed: 3:29:01.380602\n",
            "Epoch [76/84](100.00%)\t                            valid loss: 0.9297\t                            valid acc: 71.5072%\t                            time elapsed: 3:29:05.555152\n",
            "====================================================================================================\n",
            "Epoch [77] Learning Rate : 0.00020625\n",
            "Epoch [77/84](0.74%)\t                        train loss: 0.4231\t                        train acc: 89.0625%\t                        time elapsed: 3:29:06.808185\n",
            "Epoch [77/84](37.78%)\t                        train loss: 0.4399\t                        train acc: 87.2855%\t                        time elapsed: 3:30:04.277908\n",
            "Epoch [77/84](74.81%)\t                        train loss: 0.4497\t                        train acc: 87.0359%\t                        time elapsed: 3:31:02.606181\n",
            "Epoch [77/84](100.00%)\t                        train loss: 0.4529\t                        train acc: 86.8740%\t                        time elapsed: 3:31:41.767496\n",
            "====================================================================================================\n",
            "Epoch [77/84](12.50%)\t                            valid loss: 1.1597\t                            valid acc: 68.7500%\t                            time elapsed: 3:31:42.460767\n",
            "Epoch [77/84](100.00%)\t                            valid loss: 0.9258\t                            valid acc: 71.8372%\t                            time elapsed: 3:31:46.655932\n",
            "====================================================================================================\n",
            "Epoch [78] Learning Rate : 0.00020625\n",
            "Epoch [78/84](0.74%)\t                        train loss: 0.3778\t                        train acc: 88.2812%\t                        time elapsed: 3:31:47.903649\n",
            "Epoch [78/84](37.78%)\t                        train loss: 0.4269\t                        train acc: 87.6072%\t                        time elapsed: 3:32:46.041008\n",
            "Epoch [78/84](74.81%)\t                        train loss: 0.4408\t                        train acc: 87.2602%\t                        time elapsed: 3:33:43.960861\n",
            "Epoch [78/84](100.00%)\t                        train loss: 0.4412\t                        train acc: 87.3835%\t                        time elapsed: 3:34:23.586446\n",
            "====================================================================================================\n",
            "Epoch [78/84](12.50%)\t                            valid loss: 1.1541\t                            valid acc: 70.3125%\t                            time elapsed: 3:34:24.281915\n",
            "Epoch [78/84](100.00%)\t                            valid loss: 0.9264\t                            valid acc: 73.3773%\t                            time elapsed: 3:34:28.447878\n",
            "====================================================================================================\n",
            "Epoch [79] Learning Rate : 0.00020625\n",
            "Epoch [79/84](0.74%)\t                        train loss: 0.4154\t                        train acc: 89.0625%\t                        time elapsed: 3:34:29.691010\n",
            "Epoch [79/84](37.78%)\t                        train loss: 0.4367\t                        train acc: 87.1630%\t                        time elapsed: 3:35:27.150378\n",
            "Epoch [79/84](74.81%)\t                        train loss: 0.4424\t                        train acc: 86.9508%\t                        time elapsed: 3:36:25.125244\n",
            "Epoch [79/84](100.00%)\t                        train loss: 0.4443\t                        train acc: 86.9087%\t                        time elapsed: 3:37:04.150225\n",
            "====================================================================================================\n",
            "Epoch [79/84](12.50%)\t                            valid loss: 1.1810\t                            valid acc: 67.1875%\t                            time elapsed: 3:37:04.822384\n",
            "Epoch [79/84](100.00%)\t                            valid loss: 0.9485\t                            valid acc: 72.3872%\t                            time elapsed: 3:37:08.984492\n",
            "====================================================================================================\n",
            "Epoch [80] Learning Rate : 0.00020625\n",
            "Epoch [80/84](0.74%)\t                        train loss: 0.4356\t                        train acc: 89.8438%\t                        time elapsed: 3:37:10.204090\n",
            "Epoch [80/84](37.78%)\t                        train loss: 0.4273\t                        train acc: 87.8370%\t                        time elapsed: 3:38:08.347849\n",
            "Epoch [80/84](74.81%)\t                        train loss: 0.4379\t                        train acc: 87.5000%\t                        time elapsed: 3:39:06.089559\n",
            "Epoch [80/84](100.00%)\t                        train loss: 0.4411\t                        train acc: 87.3140%\t                        time elapsed: 3:39:46.261953\n",
            "====================================================================================================\n",
            "Epoch [80/84](12.50%)\t                            valid loss: 1.1313\t                            valid acc: 72.6562%\t                            time elapsed: 3:39:46.947845\n",
            "Epoch [80/84](100.00%)\t                            valid loss: 0.9204\t                            valid acc: 73.4873%\t                            time elapsed: 3:39:51.222783\n",
            "====================================================================================================\n",
            "Epoch [81] Learning Rate : 0.00020625\n",
            "Epoch [81/84](0.74%)\t                        train loss: 0.3320\t                        train acc: 92.1875%\t                        time elapsed: 3:39:52.503816\n",
            "Epoch [81/84](37.78%)\t                        train loss: 0.4279\t                        train acc: 87.5613%\t                        time elapsed: 3:40:50.459107\n",
            "Epoch [81/84](74.81%)\t                        train loss: 0.4397\t                        train acc: 87.0282%\t                        time elapsed: 3:41:48.740931\n",
            "Epoch [81/84](100.00%)\t                        train loss: 0.4423\t                        train acc: 87.0477%\t                        time elapsed: 3:42:27.638397\n",
            "====================================================================================================\n",
            "Epoch [81/84](12.50%)\t                            valid loss: 1.1297\t                            valid acc: 69.5312%\t                            time elapsed: 3:42:28.335363\n",
            "Epoch [81/84](100.00%)\t                            valid loss: 0.9308\t                            valid acc: 72.0572%\t                            time elapsed: 3:42:32.573167\n",
            "====================================================================================================\n",
            "Epoch [82] Learning Rate : 0.00020625\n",
            "Epoch [82/84](0.74%)\t                        train loss: 0.5018\t                        train acc: 88.2812%\t                        time elapsed: 3:42:33.829858\n",
            "Epoch [82/84](37.78%)\t                        train loss: 0.4148\t                        train acc: 88.3885%\t                        time elapsed: 3:43:32.021809\n",
            "Epoch [82/84](74.81%)\t                        train loss: 0.4216\t                        train acc: 88.0260%\t                        time elapsed: 3:44:29.246807\n",
            "Epoch [82/84](100.00%)\t                        train loss: 0.4278\t                        train acc: 87.8004%\t                        time elapsed: 3:45:08.544558\n",
            "====================================================================================================\n",
            "Epoch [82/84](12.50%)\t                            valid loss: 1.1202\t                            valid acc: 72.6562%\t                            time elapsed: 3:45:09.208771\n",
            "Epoch [82/84](100.00%)\t                            valid loss: 0.9217\t                            valid acc: 72.9373%\t                            time elapsed: 3:45:13.338894\n",
            "====================================================================================================\n",
            "Epoch 00083: reducing learning rate of group 0 to 1.0312e-04.\n",
            "Epoch [83] Learning Rate : 0.000103125\n",
            "Epoch [83/84](0.74%)\t                        train loss: 0.5274\t                        train acc: 85.9375%\t                        time elapsed: 3:45:14.621866\n",
            "Epoch [83/84](37.78%)\t                        train loss: 0.4256\t                        train acc: 87.6225%\t                        time elapsed: 3:46:13.162723\n",
            "Epoch [83/84](74.81%)\t                        train loss: 0.4181\t                        train acc: 87.9409%\t                        time elapsed: 3:47:10.446132\n",
            "Epoch [83/84](100.00%)\t                        train loss: 0.4198\t                        train acc: 87.8525%\t                        time elapsed: 3:47:50.291926\n",
            "====================================================================================================\n",
            "Epoch [83/84](12.50%)\t                            valid loss: 1.1595\t                            valid acc: 69.5312%\t                            time elapsed: 3:47:50.971470\n",
            "Epoch [83/84](100.00%)\t                            valid loss: 0.9287\t                            valid acc: 73.4873%\t                            time elapsed: 3:47:55.119780\n",
            "====================================================================================================\n",
            "Epoch [84] Learning Rate : 0.000103125\n",
            "Epoch [84/84](0.74%)\t                        train loss: 0.3720\t                        train acc: 89.8438%\t                        time elapsed: 3:47:56.321192\n",
            "Epoch [84/84](37.78%)\t                        train loss: 0.3935\t                        train acc: 88.7868%\t                        time elapsed: 3:48:53.795910\n",
            "Epoch [84/84](74.81%)\t                        train loss: 0.4034\t                        train acc: 88.6989%\t                        time elapsed: 3:49:52.438593\n",
            "Epoch [84/84](100.00%)\t                        train loss: 0.4059\t                        train acc: 88.5125%\t                        time elapsed: 3:50:31.213251\n",
            "====================================================================================================\n",
            "Epoch [84/84](12.50%)\t                            valid loss: 1.1437\t                            valid acc: 71.0938%\t                            time elapsed: 3:50:31.919999\n",
            "Epoch [84/84](100.00%)\t                            valid loss: 0.9373\t                            valid acc: 73.0473%\t                            time elapsed: 3:50:36.122295\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xticks(np.arange(0, 111, 10))\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(range(len(train_losses)), train_losses, label='train_loss', color='red')\n",
        "plt.plot(range(len(valid_losses)), valid_losses, label='valid_loss', color='blue')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fB-SYdQvTZfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4db6b15f-9302-4b38-b710-90f4aa6ca823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xticks(np.arange(0, 111, 10))\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(range(len(train_accs)), train_accs, label='train_acc', color='red')\n",
        "plt.plot(range(len(valid_accs)), valid_accs, label='valid_acc', color='blue')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L5GeZqVdT6JC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4eb310a0-8be1-4bdf-896a-5a600f05aef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test**"
      ],
      "metadata": {
        "id": "wazDI6Xm7zQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    preds = None\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            if preds is None:\n",
        "                preds = model(data)\n",
        "            else:\n",
        "                preds = torch.cat((preds, model(data)), dim=0)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "_MyJnRub9Jrz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = { 'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        'model_name' : 'efficientnet_lite0',\n",
        "        'num_classes' : 120,\n",
        "        'path2weight' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/weights/save_best/',\n",
        "        'path2submit' : '/content/drive/Shareddrives/KHU/Kaggle/D.COM_AI_COMPETITION_1th/submission/',\n",
        "       }\n",
        "\n",
        "path2weight = cfg['path2weight']+cfg['model_name']\n",
        "path2submit = cfg['path2submit']+cfg['model_name']+'_cross_val.csv'"
      ],
      "metadata": {
        "id": "asmunG2oyNav"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble & Hard Voting"
      ],
      "metadata": {
        "id": "zqTtusDJ_PKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "test_data = TestDataset(root='./test', transform=test_transform)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "preds_list=[]\n",
        "\n",
        "for fold in [2,5,6,7]:\n",
        "    model = timm.create_model(cfg['model_name'], pretrained=False, num_classes=cfg['num_classes']).to(cfg['device'])\n",
        "    param_check(model)\n",
        "    model.load_state_dict(torch.load(f'{path2weight}_gc01_save{fold}.pth'))\n",
        "    preds = test(model, test_loader, cfg['device'])\n",
        "    preds = preds.view(preds.shape[0], preds.shape[1], 1).cpu()\n",
        "    preds_list.append(preds)\n",
        "\n",
        "preds_arr = np.concatenate(preds_list, axis=2)\n",
        "preds_mean = preds_arr.mean(axis=2)\n",
        "preds_mean = torch.from_numpy(preds_mean)\n",
        "preds = torch.argmax(preds_mean, 1)\n",
        "\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission['Category'] = preds.cpu().numpy()\n",
        "submission.to_csv(path2submit, index=False)\n",
        "print(\"ensemble finish.\")"
      ],
      "metadata": {
        "id": "HxnAj4YbqmvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99db9d11-bbb4-4301-8fcb-dddec50f5e46"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of parameters : 3524728\n",
            "\n",
            "# of parameters : 3524728\n",
            "\n",
            "# of parameters : 3524728\n",
            "\n",
            "# of parameters : 3524728\n",
            "\n",
            "ensemble finish.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "2nd_이의준_73_5",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}